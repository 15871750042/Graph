## 图卷积神经网络GCN

![截屏2023-03-13 下午7.45.50](https://p.ipic.vip/55l63x.png)

![截屏2023-03-13 下午7.48.39](https://p.ipic.vip/4dp2rl.png)

![截屏2023-03-13 下午7.49.05](https://p.ipic.vip/okr4zy.png)

### 图卷积神经网络-计算图

![截屏2023-03-13 下午7.50.17](https://p.ipic.vip/jmrvnj.png)

![截屏2023-03-13 下午7.50.41](https://p.ipic.vip/83m0af.png)

![截屏2023-03-13 下午7.50.59](https://p.ipic.vip/xvxxqs.png)

![截屏2023-03-13 下午7.51.39](https://p.ipic.vip/vctfvl.png)

![截屏2023-03-13 下午7.51.21](https://p.ipic.vip/8e2igb.png)

黑盒子和白盒子中神经网络层数不是图神经网络的层数。

![截屏2023-03-13 下午7.53.31](https://p.ipic.vip/wltbeb.png)

注意：图神经网络不能过深，一般两三层即可，如果过深例如超过6层，根据六度空间理论可知，最终所有节点的计算图都很类似，即过平滑。

![截屏2023-03-13 下午7.58.37](https://p.ipic.vip/mpqwwb.png)

首先，将每个节点（128\*1）进行逐元素求平均，得到128\*1的向量，然后通过黑盒子的神经网络，输出512\*1的向量，再进行白黑子的神经网络，输出256\*1的向量。

![截屏2023-03-13 下午8.08.11](https://p.ipic.vip/gdfdvx.png)

需要训练每个神经网络的权重，首先需要将**反向传播**训练得到神经网络权重，然后进行**前向传播**进行预测。

![截屏2023-03-13 下午8.12.02](https://p.ipic.vip/1i8g7j.png)

### 图卷积神经网络-数学形式

![截屏2023-03-13 下午8.13.42](https://p.ipic.vip/m5n7px.png)

注意：蓝框中的计算公式，就是进行逐元素求平均的过程，并且这个过程具有顺序不变形。

![截屏2023-03-13 下午8.16.55](https://p.ipic.vip/us8v4y.png)

注意：上面A表示邻接矩阵，$H^{(k)}$表示第k层的所有节点的属性，然后$A*H^{(k)}$表示将第k层的所有节点的属性求和，然后$D^{(-1)}AH^{(k)}$就是对第k层的所有节点属性进行逐元素求平均。

![截屏2023-03-13 下午8.34.36](https://p.ipic.vip/eyambk.png)

![截屏2023-03-13 下午8.34.51](https://p.ipic.vip/odk7xc.png)

![截屏2023-03-13 下午8.37.56](https://p.ipic.vip/jy3o8d.png)

上面没考虑邻居节点的连接数，即未考虑权重信息。

![截屏2023-03-13 下午8.38.16](https://p.ipic.vip/lgnmqm.png)

上面虽然考虑的节点的连接数，但是幅值变小，就需要引入下面的方法。

![截屏2023-03-13 下午8.40.20](https://p.ipic.vip/nhrpbz.png)

![截屏2023-03-13 下午8.43.46](https://p.ipic.vip/a4jt0x.png)

![截屏2023-03-13 下午8.44.07](https://p.ipic.vip/u70ye2.png)

![截屏2023-03-13 下午8.47.34](https://p.ipic.vip/yz3i1v.png)

![截屏2023-03-13 下午8.47.56](https://p.ipic.vip/ltuqeh.png)

![截屏2023-03-13 下午8.48.25](https://p.ipic.vip/eeghz3.png)

![截屏2023-03-13 下午8.49.31](https://p.ipic.vip/fag35f.png)

![截屏2023-03-13 下午8.49.46](https://p.ipic.vip/m8gruu.png)

![截屏2023-03-13 下午8.50.07](https://p.ipic.vip/0hwqwg.png)

### 图卷积神经网络-计算图 改进（‼️）

之前计算图中，仅参考别的节点属性，而未考虑自身节点的属性，最终计算图改进如下图所示：

![截屏2023-03-13 下午8.52.24](https://p.ipic.vip/96h81u.png)

![截屏2023-03-13 下午8.53.17](https://p.ipic.vip/cnzvab.png)

![截屏2023-03-13 下午8.53.50](https://p.ipic.vip/6177l7.png)

![截屏2023-03-13 下午8.58.00](https://p.ipic.vip/bt0983.png)

![截屏2023-03-13 下午8.59.07](https://p.ipic.vip/dabbhc.png)

![截屏2023-03-13 下午8.58.44](https://p.ipic.vip/nqyyez.png)

### 如何训练神经网络？

![截屏2023-03-13 下午9.00.17](https://p.ipic.vip/7fwfdu.png)

![截屏2023-03-13 下午9.00.54](https://p.ipic.vip/dr3bo2.png)

![截屏2023-03-13 下午9.01.10](https://p.ipic.vip/6xe481.png)

![截屏2023-03-13 下午9.01.48](https://p.ipic.vip/2geo79.png)

![截屏2023-03-13 下午9.03.02](https://p.ipic.vip/sg4ita.png)

### 图神经网络相比传统方法的优点

![截屏2023-03-13 下午9.05.51](https://p.ipic.vip/dtot5b.png)

![截屏2023-03-13 下午9.06.29](https://p.ipic.vip/8jfpss.png)

![截屏2023-03-13 下午9.07.25](https://p.ipic.vip/3245pc.png)

![截屏2023-03-13 下午9.07.46](https://p.ipic.vip/5id792.png)

![截屏2023-03-13 下午9.08.19](https://p.ipic.vip/912gwz.png)

![截屏2023-03-13 下午9.09.31](https://p.ipic.vip/kkv1tr.png)

![截屏2023-03-13 下午9.09.56](https://p.ipic.vip/q220z3.png)

![截屏2023-03-13 下午9.10.12](https://p.ipic.vip/1xpuia.png)

![截屏2023-03-13 下午9.10.45](https://p.ipic.vip/no4lxa.png)

![截屏2023-03-13 下午9.11.43](https://p.ipic.vip/2gqjlc.png)

![截屏2023-03-13 下午9.11.01](https://p.ipic.vip/1cqiw8.png)

![截屏2023-03-13 下午9.12.46](https://p.ipic.vip/f50rcj.png)

![截屏2023-03-13 下午9.13.15](https://p.ipic.vip/vn8pk7.png)

传统方法没有使用到节点自身属性和标注信息，图神经网络的时间呈线性增长。

![截屏2023-03-13 下午9.14.51](https://p.ipic.vip/lsyou9.png)

![截屏2023-03-13 下午9.15.48](https://p.ipic.vip/3y85s3.png)

![截屏2023-03-13 下午9.16.49](https://p.ipic.vip/nmlrel.png)

![截屏2023-03-13 下午9.17.04](https://p.ipic.vip/qlzvrl.png)



![截屏2023-03-13 下午9.19.27](https://p.ipic.vip/cgomeb.png)

![截屏2023-03-13 下午9.19.43](https://p.ipic.vip/1ovik6.png)

![截屏2023-03-13 下午9.20.34](https://p.ipic.vip/yxkcu2.png)

![截屏2023-03-13 下午9.21.51](https://p.ipic.vip/xduemx.png)

上图中cnn不具有置换不变性，但是gcn具有**置换不变性**。

![截屏2023-03-13 下午9.22.31](https://p.ipic.vip/ywwl0s.png)

![截屏2023-03-13 下午9.22.11](https://p.ipic.vip/fzq458.png)

### 复盘总结

![截屏2023-03-13 下午9.24.36](https://p.ipic.vip/uyzrn2.png)

![截屏2023-03-13 下午9.24.51](https://p.ipic.vip/3dxawi.png)

![截屏2023-03-13 下午9.25.08](https://p.ipic.vip/gzqnsq.png)

![截屏2023-03-13 下午9.26.02](https://p.ipic.vip/02pmfs.png)

### 思考题

**1.GCN的计算图是如何构建的？**

GCN的计算图是通过局部领域构建计算图，对局部邻域中每个节点的进行逐行求均值得到向量，然后将向量传入到神经网络中，不断迭代得到计算图。

**2.图神经网络的层数是如何计算的？**

图神经网络的层数是计算图的层数，而不是神经网络的层数。

**3.神经网络层数越多，图神经网络也越深吗？**

不会，图神经网络层数是计算图的层数，而不是神经网络的层数。

**4.理论上图神经网络可以任意深，实际上可行吗？**

不可行，当图神经网络层数较深时（超过6），依据六度空间理论知道，每个节点会连接到所有节点，则每个节点的计算图完全相同的，即过平滑。

**5.GCN的聚合函数是什么？**

逐元素求平均

**6.简述GCN的数学形式**
$$
h_v^{(0)}=x_v\\
h_v^{(k+1)}=\sigma(W_k\sum_{u\in N(v)}\frac{h_u^{k}}{|N(v)|})\\
z_v=h_v^{K}
$$
**7.简述Normalized Adjacency Matrix的推导过程**

首先，基于图中节点信息构建邻接矩阵A;

其次，基于图中每个节点的连接数得到对角矩阵D;

最后，计算$\tilde{A}=D^{-1/2}AD^{-1/2}$得到归一化的邻接矩阵。

注意：如果i和j相连，则$\tilde{A}$中$\tilde{A_{ij}}$是$\frac{A_{ij}}{\sqrt{d_i}\sqrt{d_j}}$，$d_i$表示第i个节点的连接数。

**8.为什么要引入Self Embedding？**

由于之前的节点计算图仅参考邻居节点的信息，而未参考节点自身属性信息，会导致部分信息丢失，所以引入self embedding。

**9.“图卷积”和“图像卷积”有什么异同？**



**10.如何通过监督学习的方式训练图神经网络？**（‼️）

首先，对部分已知节点进行标注，构建节点的计算图；

其次，将每个计算图作为样本，输入到图神经网络中，得到一个嵌入向量；

然后，将嵌入向量接一个分类头，获得预测结果；

最后，将预测结果与标注结果进行比对，计算交叉熵损失函数，目标将参数进行微调，使得交叉熵损失函数最小；

**11.如何通过无监督（自监督）学习的方式训练图神经网络？**（‼️）

将两个节点分别输入到图神经网络中，得到d维向量，计算两个向量的点乘/数量积/余弦相似度等运算（例如：通过余弦相似度反映两个节点在原图的关系，余弦相似度越大则节点越相似）获取一个编码器/解码器，使得损失函数最小化。

最简单的编码器：查表。

注意：定义两个节点相似度（随机游走在相同序列、两个节点相连）

**12.为什么图神经网络具有归纳式学习能力？**

归纳式学习是用于预测的节点在训练时没见过（泛化到新节点），而图神经网络中，在新节点与当前节点建立连接，就可以获得新节点的属性特征和计算图，就可以运行已经训练好的图神经网络，获取其嵌入向量，所以图神经网络具有归纳式学习能力。

**13.图神经网络的参数量如何计算？**

每层神经网络中的参数量是共享的，所以只需要将计算图中每层参数求和即可。

**14.图神经网络具有哪些优点？**

- **深度学习拟合学习能力强**，表示学习得到的嵌入向量质量高；
- **归纳式学习能力**：可以泛化到新节点、新图；
- 每个节点的嵌入向量不需要单独训练，**参数量少**，所有计算图共享图神经网络（参数共享）；
- 使用了节点**属性特征/标注信息**；
- 能区分节点**结构功能角色**（桥接、中枢、外围边缘）；
- 只需寥寥几层，就能够让任意两个节点相互影响。

**15.GCN能否使用节点的属性？为什么？**

能，因为GCN中计算图是根据节点局部领域属性得到计算图，然后传入到图神经网络中，得到嵌入向量。

**16.GCN能否使用连接的属性？为什么？**

否

**17.GCN还具有哪些缺点？（表达能力、聚合函数）**

由于GCN中聚合函数不是单射函数，所以不同结构的图最终得到的嵌入向量可能相同，所以GCN的表达能力不太好。

**18.GCN可以解决哪些图机器学习问题？**

节点分类

**19.如何理解“卷积神经网络是图神经网络的特例”？**
CNN可以视作拥有固定邻域和固定顺序的GNN，CNN不是置换不变性，但是GCN是置换不变性。

**20.如何理解“Transformer是图神经网络的特例”？**（❓）

Transformer是自注意力机制，可以看作全连接词图上的GNN。