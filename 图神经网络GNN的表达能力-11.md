## 图神经网络GNN的表达能力

### GNN

![截屏2023-03-13 下午9.38.26](https://p.ipic.vip/3a7w7z.png)

图神经网络的应用：

- 基于节点嵌入，加一个预测头，进行节点分类；
- 两个节点的嵌入，加上预测头，进行链接预测；
- 将所有节点的嵌入汇总在一起，做readout变成全图的嵌入，进行全图分类；

![截屏2023-03-19 下午10.27.35](https://p.ipic.vip/f3najs.png)

![截屏2023-03-13 下午9.41.58](https://p.ipic.vip/kopxn0.png)

![截屏2023-03-13 下午9.42.33](https://p.ipic.vip/7ujy1v.png)

将每个节点的嵌入汇总，进行readout操作，得到全图嵌入，进行全图分类。

![截屏2023-03-13 下午9.46.26](https://p.ipic.vip/z7yk5s.png)

高维空间中非线性非凸

### **理论基础：万能近似定理**

**理论上，一个隐含层的神经网络中神经元个数足够多，就可以拟合任意一种连续函数**

![截屏2023-03-13 下午9.47.21](https://p.ipic.vip/a8jk6m.png)

实际应用：

![截屏2023-03-13 下午9.53.09](https://p.ipic.vip/n5ie9d.png)

消息传递图神经网络的框架：

![截屏2023-03-13 下午9.54.37](https://p.ipic.vip/5d1i4s.png)

先对节点邻域进行消息传递，然后进行聚合。

![截屏2023-03-13 下午9.56.33](https://p.ipic.vip/n0f0f8.png)

![截屏2023-03-13 下午9.58.00](https://p.ipic.vip/pndmzy.png)

![截屏2023-03-13 下午9.58.23](https://p.ipic.vip/z4a7yy.png)

![截屏2023-03-13 下午9.58.46](https://p.ipic.vip/2oimj3.png)

不考虑节点属性（所有节点颜色一致），而通过节点的连接结构区分不同节点。

![截屏2023-03-13 下午9.59.04](https://p.ipic.vip/rkeax7.png)

![截屏2023-03-13 下午9.59.28](https://p.ipic.vip/56inig.png)

![截屏2023-03-13 下午9.59.51](https://p.ipic.vip/57griv.png)

**计算图的本质是提取根节点的嵌入**

![截屏2023-03-13 下午10.00.04](https://p.ipic.vip/262v51.png)

![截屏2023-03-13 下午10.00.20](https://p.ipic.vip/52fnnr.png)

![截屏2023-03-13 下午10.00.33](https://p.ipic.vip/jvqm2p.png)

**计算图中：编号无意义、连接长度/朝向无意义、颜色（属性）有意义**

![截屏2023-03-13 下午10.00.51](https://p.ipic.vip/oxwjll.png)

![截屏2023-03-13 下午10.01.18](https://p.ipic.vip/w2s59s.png)

**两个基点的属性特征相同，计算图相同，embedding也相同。**

![截屏2023-03-13 下午10.01.44](https://p.ipic.vip/7c94uw.png)

![截屏2023-03-13 下午10.02.02](https://p.ipic.vip/qbfabk.png)

图神经网络的表达能力是**区分计算图根节点embedding的能力**。

![截屏2023-03-13 下午10.02.43](https://p.ipic.vip/zpnumc.png)

![截屏2023-03-13 下午10.02.28](https://p.ipic.vip/4nwsie.png)

![截屏2023-03-13 下午10.03.22](https://p.ipic.vip/gs5eh7.png)

![截屏2023-03-13 下午10.04.32](https://p.ipic.vip/8dplj3.png)

![截屏2023-03-13 下午10.04.09](https://p.ipic.vip/op4qie.png)

![截屏2023-03-13 下午10.05.18](https://p.ipic.vip/6fw9ci.png)

![截屏2023-03-13 下午10.05.37](https://p.ipic.vip/5lbssk.png)

![截屏2023-03-13 下午10.06.12](https://p.ipic.vip/qs1k61.png)

![截屏2023-03-13 下午10.06.26](https://p.ipic.vip/bwtp2o.png)

![截屏2023-03-13 下午10.07.03](/Users/jiamei5/Library/Application Support/typora-user-images/截屏2023-03-13 下午10.07.03.png)

![截屏2023-03-14 下午7.27.56](https://p.ipic.vip/6qn1nf.png)

![截屏2023-03-14 下午7.27.17](https://p.ipic.vip/zzc1kv.png)

![截屏2023-03-14 下午7.28.11](https://p.ipic.vip/35h8e9.png)

![截屏2023-03-14 下午7.58.07](https://p.ipic.vip/cnediq.png)

![截屏2023-03-14 下午7.58.28](https://p.ipic.vip/j5cvg1.png)

上图中gcn进行逐元素求平均，两个不同结构的计算图得到都是$(0.5，0.5)^{T}$，即图嵌入后的结果完全相同，因此不是单射函数。

![截屏2023-03-14 下午7.58.48](https://p.ipic.vip/ejqboc.png)

![截屏2023-03-14 下午8.04.09](https://p.ipic.vip/vgefly.png)

上图中graphsage进行逐元素求最大值，三个不同结构得到都是$(1，1)^{T}$，即图嵌入后的结果完全相同，因此也不是单射函数。![截屏2023-03-14 下午8.11.33](https://p.ipic.vip/vgakjd.png)

gcn和graphsage中都不是单射聚合函数，所以全图也不是单射函数，因此表达能力不是最强的。

![截屏2023-03-14 下午8.16.26](https://p.ipic.vip/q2v4ri.png)

![截屏2023-03-14 下午8.16.39](https://p.ipic.vip/hsxb13.png)

### GIN-graph isomorphism network

GIN的聚合操作是两个神经网络，首先对每个节点元素进行映射f(x)，然后将每层神经网络中每个节点映射后的结果求和，然后进行单射函数

![截屏2023-03-14 下午8.18.00](https://p.ipic.vip/oekiji.png)

![截屏2023-03-14 下午8.18.31](https://p.ipic.vip/2ukv00.png)

**理论基础：万能近似定理**

![截屏2023-03-14 下午8.20.02](https://p.ipic.vip/ozm9ga.png)

![截屏2023-03-14 下午8.19.40](https://p.ipic.vip/1b8wjd.png)

隐藏层的神经网络个数载100到500之间。

![截屏2023-03-14 下午8.23.07](https://p.ipic.vip/b111ij.png)

**GIN是表达能力最强的消息传递GNN**

![截屏2023-03-14 下午8.24.36](https://p.ipic.vip/f5i4u5.png)

![截屏2023-03-14 下午8.32.39](https://p.ipic.vip/bgzcqc.png)

GIN是颜色微调算法版本的神经网络。

![截屏2023-03-14 下午8.33.18](https://p.ipic.vip/gz04ic.png)

![截屏2023-03-14 下午8.33.41](https://p.ipic.vip/vy2f7e.png)

![截屏2023-03-14 下午8.35.57](https://p.ipic.vip/5cekee.png)

本质是hash操作，将自己的颜色和周围的颜色映射成新颜色，k表示几跳的邻域信息。

![截屏2023-03-14 下午8.36.29](https://p.ipic.vip/wdozo8.png)

**颜色微调**是将自己的颜色和周围节点的颜色映射成新颜色；**GIN**是将自己的嵌入和周围的嵌入映射成新嵌入。

![截屏2023-03-14 下午8.41.30](https://p.ipic.vip/l8wxbp.png)

![截屏2023-03-14 下午8.41.47](https://p.ipic.vip/e7jlaa.png)

![截屏2023-03-14 下午8.42.02](https://p.ipic.vip/dv60kc.png)

![截屏2023-03-14 下午8.43.41](https://p.ipic.vip/0cfb0d.png)

GIN是可求导可微分的神经网络。

![截屏2023-03-14 下午8.47.59](https://p.ipic.vip/qqmrdv.png)

![截屏2023-03-14 下午8.48.53](https://p.ipic.vip/0m9fme.png)

![截屏2023-03-14 下午8.50.25](https://p.ipic.vip/etkr56.png)

### GNN工程调参技巧

![截屏2023-03-14 下午8.51.58](https://p.ipic.vip/3xtkus.png)

![截屏2023-03-14 下午8.52.37](https://p.ipic.vip/hw1swr.png)

![截屏2023-03-14 下午8.52.59](https://p.ipic.vip/ezej3h.png)

pytorch优于tensorflow

![截屏2023-03-14 下午8.54.04](https://p.ipic.vip/hpuiaf.png)

![截屏2023-03-14 下午8.54.15](https://p.ipic.vip/l7bcj9.png)

![截屏2023-03-14 下午8.56.20](https://p.ipic.vip/o73wdl.png)

![截屏2023-03-14 下午9.13.48](https://p.ipic.vip/w52rf7.png)

**1.不同类型的神经网络，表达能力各有何区别？**

卷积神经网络对于图像数据的表达能力

循环神经网络对于文本、语音数据的表达能力

Transformer对序列数据的表达能力

图神经网络对图数据的表达能力

**不同类型的神经网络的表达能力处理的数据类型不同。**

**2.为什么要研究图神经网络的表达能力？**

图神经网络是一类用于图数据分析的深度学习模型，其主要应用于节点分类、链接预测、图聚类等任务。研究图神经网络的表达能力，有以下几个原因：

1. 理论上的保证：研究图神经网络的表达能力可以帮助我们理解图神经网络的优劣以及其应用的限制。在理论上，研究图神经网络的表达能力可以为我们提供理论保证，即在理论上我们可以证明该模型是否能够解决某些特定的图数据分析任务。例如，研究图神经网络的表达能力可以帮助我们理解图神经网络在表达不同类型的图结构时的能力，包括如何对图的拓扑结构和节点属性进行编码。
2. 模型改进的指导：研究图神经网络的表达能力可以帮助我们指导模型的改进。在实践中，我们可能发现某些图数据分析任务难以用现有的图神经网络模型解决，研究图神经网络的表达能力可以帮助我们理解模型存在的局限性，并设计更加有效的模型。
3. 应用拓展的支持：研究图神经网络的表达能力可以帮助我们拓展图神经网络的应用范围。在实践中，我们可能需要对不同类型的图结构进行建模，例如具有多种属性、非欧几里得空间的图等。研究图神经网络的表达能力可以帮助我们设计更加适应不同类型图结构的图神经网络模型，从而拓展其应用范围。

因此，研究图神经网络的表达能力具有重要意义，可以帮助我们深入理解图神经网络的特点和应用，指导模型的改进和应用拓展。

**3.图神经网络的表达能力如何体现？**

图神经网络的表达能力就是**区分不同图结构的能力**，图神经网络的表达能力体现在**区分计算图根节点embedding的能力**

**4.GCN的表达能力如何？**

GCN的聚合函数是求平均，不是单射函数，所以表达能力不如GIN

**5.GraphSAGE的表达能力如何？**

GraphSAGE的聚合函数是最大池化，不是单射函数，所以表达能力不如GIN

**6.GAT的表达能力如何？**

GAT（Graph Attention Network）是一种用于图结构数据的深度学习模型，其基于自注意力机制，通过计算每个节点与其邻居节点之间的注意力权重来进行信息传递和图结构表示学习。GAT具有一定的表达能力，其主要表现在以下几个方面：

1. 节点表示：GAT可以对节点进行有效的表示学习。通过计算节点与其邻居节点之间的注意力权重，GAT可以对节点的邻居节点进行加权平均，并将这个平均值作为节点的表示。由于注意力权重的计算考虑了节点之间的相关性，因此GAT可以学习到更加有效的节点表示。
2. 图表示：GAT可以有效地学习整个图的表示。通过对所有节点的表示进行加权平均，GAT可以得到整个图的表示。由于节点之间的注意力权重考虑了节点之间的相关性，因此GAT可以捕捉到整个图的拓扑结构和节点属性之间的关系。
3. 模型扩展：GAT可以通过堆叠多个注意力头来扩展模型的表达能力。每个注意力头可以学习不同的节点表示，并将这些表示进行组合。通过堆叠多个注意力头，GAT可以学习到更加复杂的节点和图表示，从而提高模型的表达能力。

总的来说，GAT具有一定的表达能力，可以用于有效地学习图结构数据的表示。虽然GAT的表达能力受到注意力机制的限制，但是通过增加注意力头的数量或使用其他的注意力机制，可以进一步提高GAT的表达能力。

**7.GIN的表达能力如何？**

GIN的聚合操作是两个神经网络，首先对每个节点元素进行映射f(x)，然后将每层神经网络中每个节点映射后的结果求和，然后进行单射函数，由于聚合操作是单射函数，表达能力优于GCN、GraphSAGE，是表达能力最强的消息传递GNN.

**8.图神经网络的表达能力，关键取决于哪一步操作？**

聚合操作，理想情况应该是单射重集函数。

**9.什么是函数的单射、满射、双射？**

单射：每个输出对应不超过一个输入

满射：所有的输出都对应输入

双射：输入和输出一一对应

**![截屏2023-03-19 下午10.08.21](https://p.ipic.vip/956l2s.png)10.求和、最大池化、求平均，三者的表达能力如何排序？**

对于神经网络的池化操作，通常有三种常用的方式：求和、最大池化和求平均。下面是它们的表达能力排序：

1. 最大池化：最大池化可以提取输入数据中的最重要的特征，因此它具有最强的表达能力。它可以过滤掉不重要的特征，只留下最有意义的特征。最大池化可以在分类、识别等任务中取得较好的表现，尤其是在图像和语音识别等领域。
2. 求平均：求平均可以捕捉输入数据的总体趋势，因此它比求和具有更强的表达能力。它可以减少输入数据的噪音，从而更好地表示数据的分布。求平均池化通常在文本分类等任务中表现良好。
3. 求和：求和是最简单、最常用的池化方法，它可以在保留所有输入数据的情况下将其压缩为一个向量，但是它的表达能力相对较弱。在某些情况下，求和池化可能无法很好地区分输入数据中的重要特征和噪音。因此，在一些特定的任务中，求和池化的表现可能不如求平均池化或最大池化。

总的来说，最大池化的表达能力最强，可以捕捉输入数据中最重要的特征，求平均的表达能力次之，可以捕捉数据的总体趋势，而求和的表达能力最弱，只能压缩数据并保留总和信息。

**11.和普通集合相比，“重集合”有什么特点？**

有重复元素的集合

**12.消息传递图神经网络的表达能力上限是什么？**

WL测试

**13.简述WL测试的基本原理？**

WL测试是一种用于比较两个图结构是否同构的经典算法。它的基本原理是**基于子结构匹配和节点标签的谱分解。**

具体来说，WL测试将每个节点的邻居节点以及其标签作为节点的子结构，并将每个子结构映射到一个向量。它然后将这些向量组合起来，形成一个用于表示整个图的谱向量。这个过程类似于图的谱分解，但是它不考虑图的拉普拉斯矩阵，而是考虑每个节点的子结构。

一旦得到了谱向量，WL测试就可以比较两个图的相似度。它首先比较它们的谱向量是否相同，如果不同，则表示两个图不同构。否则，它会递归地将每个节点的相邻子结构比较，直到所有子结构都被匹配或发现不同构的子结构为止。

总的来说，WL测试的基本原理是**将图结构表示为谱向量，并使用子结构匹配和节点标签进行比较**。这种方法非常灵活，可以用于不同类型的图数据，但是由于它的计算复杂度较高，不适用于大型图结构的比较。

**14.简述“万能近似定理”的内容，为什么它这么重要？**

万能近似定理：理论上，一个隐含层的神经网络中神经元个数足够多，就可以拟合任意一种连续函数。

万能近似定理是图神经网络的理论基础，奠定了深度学习的数学基石。

**15.GIN和GCN的表达能力有什么区别？**

由于GIN的聚合函数是单射函数，不同连接结构的节点得到根节点计算图的embedding是不同的，而GCN的聚合函数不是单射函数，所以GIN的表达能力优于GCN。

**16.如何在GIN的基础上，进一步提升GNN的表达能力？**

GIN（Graph Isomorphism Network）是一种基于图同构的图神经网络（GNN）模型，它在许多图分类和节点分类任务中表现出色。然而，GIN也有其限制，其中最主要的是其表达能力不足。

以下是一些可以进一步提高GNN表达能力的方法：

1. 更强大的聚合函数：GIN使用的聚合函数是求和函数，这种函数对于图的结构信息缺失较多的情况下表现较好，但是当图的结构信息丰富时，其表达能力较差。因此，使用更复杂的聚合函数（如门控循环单元GRU和长短期记忆网络LSTM）可以提高模型的表达能力。
2. 更复杂的非线性转换函数：GIN中使用的非线性转换函数是一个全连接层，这种函数的表达能力有限。可以考虑使用更复杂的非线性转换函数（如多层感知器MLP、卷积神经网络CNN等），以更好地捕捉图中的非线性关系。
3. 自适应性加权聚合：GIN中使用的聚合函数对所有邻居节点都赋予相同的权重，这可能会忽略不同节点对当前节点的贡献程度不同的情况。因此，可以考虑使用自适应性加权聚合（如Graph Attention Network GAT），以更好地捕捉节点之间的关系。
4. 多尺度聚合：对于一个图，不同尺度的邻居节点可以提供不同的信息，因此，使用多尺度聚合可以更好地捕捉图中的信息。
5. 图卷积网络（GCN）的改进：GCN是一种常用的GNN模型，可以通过改进GCN的聚合函数和非线性转换函数等方面来提高其表达能力。例如，使用类似于GIN的聚合函数，使用自适应性加权聚合等方法。

需要注意的是，不同的任务可能需要不同的模型结构和参数设置，因此，对于具体任务的模型选择需要进行实验验证。