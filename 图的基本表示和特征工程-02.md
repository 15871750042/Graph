## 图的基本表示

### 图的相关知识点

- **传统数据分析/机器学习**中**样本是独立同分布**的；**图神经网络**中**数据间、样本间是相互联系**的，所以不能使用传统的神经网络方法；

- 图神经网络是**端到端的表示学习**，可以**自动学习图中的信息**，不需要人进行干预；

- 节点、连接、子图、图都可以有特征，都可以进行数据挖掘；

### 图的基本表示

- 图的本体设计（取决于目的）
- 图的种类（有向、无向、异质（不同类型的节点/连接的图）、二分（两种节点的图，例如：演员和电影，作者和论文等）、连接带权重）
- 节点连接数（节点：nodes；连接：edges；图：graph) --G(N,E)
- 图的基本表示-邻接矩阵
- 图的基本表示-连接列表和邻接列表
- 图的连通性

### 节点的度

节点的度：节点连接数

无向图的平均度：$\bar{k}=<k>=\frac{1}{N}\sum_{i=1}^{N}k_i=\frac{2E}{N}$

有向图的平均度：$\bar{k}=\frac{E}{N}$

有向图的入度和出度平均值关系：$\bar{k^{in}}=\bar{k^{out}}$

注意：入度表示指向节点的连接数；出度表示节点指出的连接数；

![截屏2023-03-05 下午10.57.20](https://p.ipic.vip/9roqcg.png)

### 图的邻接矩阵

![截屏2023-02-15 下午8.38.59](https://p.ipic.vip/v7fwoe.png)

邻接矩阵：使用矩阵表示节点之间的连接关系，在这个矩阵中，行和列都代表顶点。该矩阵用1或0填充。这里，1表示从行顶点到列顶点有一条边，0表示从行顶点到列顶点没有边。无向图中，当两节点之间连接，则对应位置为1，否则为0；有向图中，该节点指向其它节点，对应行列位置为1，否则为0；

无向图的邻接矩阵是对称阵；

有向图的邻接矩阵是非对称阵；

![截屏2023-03-05 下午11.00.16](https://p.ipic.vip/zy0mlq.png)

无向图中，将邻接矩阵按行或按列求和，就得到节点的度（节点连接数）；

有向图中，将邻接矩阵按行求和，得到节点的出度（该节点指向的节点数），按列求和，得到节点的入度（指向该节点的节点数）；

当数据量很大的时候，邻接矩阵将非常占内存；

思考：为什么要用矩阵进行表示图？

http://www.btechsmartclass.com/data_structures/graph-representations.html

### 图的连接列表

图的连接列表：只记录存在连接的节点对

![截屏2023-02-15 下午8.44.25](https://p.ipic.vip/y4fi7w.png)

### 图的邻接列表

图的邻接列表：只记录每个节点及其相连的节点

![截屏2023-02-15 下午8.46.03](https://p.ipic.vip/otcjnu.png)

### 无权图、有权图等

![截屏2023-02-15 下午8.49.46](https://p.ipic.vip/bq7x6m.png)

![截屏2023-02-15 下午9.01.47](https://p.ipic.vip/911t44.png)

### 连通图

任意两个节点都可以触达即为**连通图**；

不能实现任意两个节点触达即为**非连通图**；

有向图中，任意两个节点可以相互触达即为**强连通图**；

在无向图中，任意两个节点可以相互触达即为**弱连通图**；

一个不连通的图可以有两个或多个连通域（connected components）组成；

![截屏2023-02-15 下午9.19.55](https://p.ipic.vip/ntp7zz.png)

### 思考题

**1.打开你的手机，里面那些APP用到了图机器学习和图神经网络的技术？（内容个性化推荐、社交网络、银行金融）**

微博中的用户加关注、点赞、评论等

qq中推荐可能认识的人

携程中邀请好友加速

**2.A股、港股、美股市值最高的上市公司，哪些公司的核心资产是图？**

Facebook、微信、推特、微博等

**3.观看电影《社交网络》，图和图数据挖掘的商业价值体现在哪些方面？**

可以基于图中节点之间关联关系进行个性化推荐

**4.马化腾在2022年12月内部讲话提到，微信视频号是整个腾讯的希望，请从图的角度解释这句话。**

微信视频号通过微信用户间的强大关系网，进行个性化推荐，可以实现视频号快速变现

**5.在你自己的研究领域，哪些数据可以用图或者网络来表示，如何进行图数据挖掘？**

微博用户间加关注、点赞数据，通过用户点赞内容的相似性，构建用户间的相似度关系，再通过社区发现算法（lpa/louvain)挖掘出用户感兴趣群体分类

**6.近年来，图数据挖掘在哪些领域带来了革命性进展？**

生物医疗领域（蛋白质结构预测）

**7.图数据挖掘解决哪些基本任务？**

- 节点层面（信用卡欺诈）
- 社区发现/子图层面（用户聚类）
- 连接层面（推荐可能认识的人）
- 图层面（分子是否有毒）

**8.分别从图、连接、节点三个层面，举例解释图数据挖掘在生物医学方面的应用。**

通过图上分子（节点）之间的作用关系（连接）判断最终药物是否对机体有毒（图）

**9.图神经网络为什么是端到端的？为什么不需要人工做特征工程？**（‼️）

![截屏2023-03-05 下午10.26.36](https://p.ipic.vip/y2jy6d.png)

1）图神经网络（Graph Neural Networks，GNN）是一种深度学习模型，用于处理图形数据，如社交网络、化学分子等。GNN是**端到端**的，因为它们**可以直接从原始图形数据中学习特征表示和分类/回归任务，而不需要先手工提取特征**。

在传统的机器学习方法中，通常需要手动提取特征，并将其输入到分类或回归模型中。但是，对于图形数据，手动提取特征是非常困难的。GNN通过在图形结构中学习节点和边的特征表示，可以直接从原始图形数据中学习特征表示。

在端到端的GNN模型中，输入是原始图形数据，输出是分类或回归结果。通过对整个模型进行端到端训练，可以使模型自动学习图形数据中的特征表示和任务之间的关系，从而获得更好的性能。

2）因为图神经网络是**通过图表示学习自动获取特征，而不需要人工做特征工程。**

**10.图神经网络和其它神经网络有什么区别？**（‼️）

图神经网络（Graph Neural Networks, GNN）与其他神经网络主要的区别在于其输入数据是图（Graph）形式的，而其他神经网络的输入数据通常是向量、矩阵或张量等。此外，GNN模型的结构也具有一些与其他神经网络不同的特点。

具体来说，以下是GNN与其他神经网络的区别：

1. **输入数据的形式**：GNN模型的输入数据是图形数据，包含**节点和边的信息**，而其他神经网络的输入通常是**向量、矩阵或张量**等。
2. **图形数据的结构性**：GNN模型考虑了图形数据的结构性，即**节点和边之间的关系**，而其他神经网络通常**不考虑输入数据之间的结构性信息**。
3. **层间信息传递的方式**：在GNN中，**每个节点会接收来自其相邻节点的信息并将其合并以生成新的节点表示**，而其他神经网络通常使用**全连接层或卷积层等方式进行信息传递**。
4. **可扩展性**：GNN模型能够很好地适应不同的图形数据结构，并具有很好的**通用性和迁移能力**，而其他神经网络通常需要针对不同的数据结构重新设计模型。

总之，GNN模型是一种专门用于处理图形数据的神经网络模型，它具有针对图形数据的独特特点和优势，如可处理非欧几里得结构、具有结构信息等。

**传统数据分析中数据间是独立同分布的，而图中数据间、样本间是相互联系的，所以在图中无法使用传统神经网络。**

**11.简述AlphaFold的基本原理，它解决了哪些以前解决不了的问题？**

把多肽链的每个残基作为节点，残基之间的角度和距离作为连接，进行图神经网络的预测，预测蛋白质结构

**12.图机器学习和传统机器学习有什么区别和难点？**（‼️）

1）**数据分布不一致**；

传统数据机器学习中都会假定数据样本是**独立同分布**的，而在图机器学习中节点与节点之间存在连接关系，所以**节点间并非完全独立**；

2）**数据类型不一致**；

图机器学习和传统机器学习的区别在于**数据类型**。传统机器学习通常处理结构化数据，如表格和向量，而图机器学习则处理非结构化数据，如图形和网络。因此，图机器学习需要开发新的算法和技术来处理这种类型的数据。

难点包括以下几个方面：

1. **数据表示**：如何将图形和网络表示为可供机器学习算法使用的数学对象。
2. **特征提取**：如何从图形和网络中提取有用的特征，这对于许多图机器学习任务是关键的。
3. **数据量和计算复杂度**：非结构化数据通常比结构化数据更大，更复杂，因此需要更多的计算资源和更高效的算法。
4. **模型选择**：选择适当的模型来解决图机器学习问题是关键，因为一些模型可能只适用于特定类型的图形和网络。
5. **应用领域**：图机器学习应用于许多领域，如社交网络分析、推荐系统、计算机视觉和自然语言处理等。因此，需要深入了解这些应用领域以确定最佳方法和算法。

总之，图机器学习是一个新兴的领域，需要处理非结构化数据，并开发新的算法和技术。这需要更多的研究和实践来解决这些挑战，并将其应用于实际问题中。

**13.图机器学习的编程工具有哪些？看看它们的官网吧（Graphgym、pyG、networkx、dgl、Pytorch、AntV、Echarts）** 

networkx、pyG、DGL、Echarts、AntV



## 图的基本表示--节点/连接/子图

传统图机器学习（人工特征工程+机器学习）

### 节点层面的特征工程

#### 半监督节点分类

- 节点的度
- 节点的重要度/中心性（邻接矩阵按行/按列求和得到重要度）
- 节点的集聚系数
- 节点周围的子图

#### 节点连接数（度）

![截屏2023-02-16 下午9.41.09](https://p.ipic.vip/s13yvc.png)

#### 中心性（重要度）分类

https://www.jsums.edu/nmeghanathan/files/2015/08/CSC641-Fall2015-Module-2-Centrality-Measures.pdf?x61976

https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html

##### Degree centrality

**节点的连接数**作为**节点的重要度**，将邻接矩阵进行行或列求和，仅看节点连接数，未考虑相邻节点的重要程度。

![截屏2023-02-27 下午7.32.34](https://p.ipic.vip/z3lis1.png)

##### Eigenvector centrality

**特征向量节点重要度（节点相邻节点的重要度）**，其为邻接矩阵的特征向量。（递归过程）

![截屏2023-02-15 下午9.40.35](https://p.ipic.vip/93ltwm.png)

![截屏2023-02-15 下午9.41.06](https://p.ipic.vip/26didf.png)

特征向量重要度比度重要度更偏向真正重要的节点（连接边很多不一定很重要），**可能不收敛；**

中心性c是邻接矩阵A特征值为1的特征向量。

pagerank和Katz centrality 是特征向量中心性的变体。

##### Betweenness centrality

判断节点是否处在必经之路，必须经过某个节点的最短路径数量占全部最短路径数据的比例。

![截屏2023-02-16 下午9.50.19](https://p.ipic.vip/80sutu.png)

##### Closeness centrality

表示去哪儿都近的节点（处在图的中心），某个节点到其他节点最短路径长度求和的倒数。

![截屏2023-02-16 下午9.52.33](https://p.ipic.vip/tb0gjm.png)

#### 集群系数 Clustering Coefficient

集群系数是衡量节点周围的抱团情况，某个节点相邻节点两两相连的个数（三角形个数）与某个节点相邻节点组合数（不考虑顺序）的比值。

clusetring coefficient：计算某个节点邻居节点两两相邻路径条数与该节点邻居节点两两组合数的比值，或者计算某个节点邻居节点组成三角形个数与某个节点邻居节点可以组成三角形个数的比值。（邻居节点抱团情况）

![截屏2023-02-16 下午9.54.02](https://p.ipic.vip/d6q9so.png)

#### 节点周围的子图

**自我中心网络**：网络节点由唯一的一个中心节点（ego)，以及这个节点的邻居（alters）组成，边只包括ego与alter之间，以及alter与alter之间的边。

![截屏2023-02-16 下午9.57.42](https://p.ipic.vip/jn9trr.png)

![截屏2023-02-16 下午9.59.12](https://p.ipic.vip/i3jiy9.png)

![截屏2023-02-16 下午9.59.50](https://p.ipic.vip/j9kb5d.png)

![截屏2023-02-16 下午10.01.09](https://p.ipic.vip/wi160f.png)

![截屏2023-02-27 下午9.15.18](https://p.ipic.vip/hzx1fj.png)

#### 总结

![截屏2023-02-16 下午10.01.32](https://p.ipic.vip/t3dtxu.png)

![截屏2023-02-27 下午9.16.31](https://p.ipic.vip/9ou4ii.png)

#### 思考题--节点

**1.节点层面，存在哪些数据挖掘任务，有何应用场景？**

寻找关键节点，例如：

社交场景下寻找最具影响力的人

学术领域最具影响力的人

**2.“传统图机器学习方法”传统在何处？**

体现在需要人工进行特征工程

**3.特征工程在数据挖掘中有什么作用？**

特征工程在数据挖掘中决定了数据质量，进而影响最终结论

**4.在传统图机器学习中，为什么要对节点、连接、全图做特征工程？**

因为机器学习只能对数值型特征进行分析处理，而无法直接对图进行分析处理，通过对节点、连接、全图做特征工程，得到数值型的矩阵或向量，就可以通过传统机器学习方法进行分析。

**5.传统图机器学习方法相比图神经网络（深度学习），有什么优点和缺点？**（‼️）

传统图机器学习方法和图神经网络（深度学习）方法都被用于图数据的建模和分析。它们有各自的优点和缺点。

传统图机器学习方法的优点包括：

1. **可解释性**：传统图机器学习方法通常使用手工设计的特征来表示节点和边。这些特征的物理含义和意义是清晰的，可以帮助解释学习到的模型。

2. **鲁棒性**：传统图机器学习方法通常不需要大量的标注数据来训练模型，因此它们对于噪声和稀疏数据比较鲁棒。

3. **可扩展性**：传统图机器学习方法可以通过添加新的特征来扩展模型，这使得它们可以用于处理不同类型的图数据。

传统图机器学习方法的缺点：

1. 特征工程复杂：在传统图机器学习方法中，需要手动设计特征，因此需要专业的领域知识和经验。
2. 鲁棒性差：传统图机器学习方法对于噪声数据比较敏感，容易产生过拟合现象。
3. 无法自适应学习：传统图机器学习方法无法自动学习特征表示，因此需要手动设计特征，而手动设计特征可能会受到领域知识和经验的限制。

与传统图机器学习方法相比，图神经网络（深度学习）的优点：

1. 自适应学习：图神经网络可以自动学习节点和边的特征表示，避免了手动设计特征的复杂过程。
2. 鲁棒性好：图神经网络可以通过层次化的特征表示和正则化方法来提高模型的鲁棒性。
3. 泛化能力强：图神经网络具有很强的泛化能力，可以适应不同的图数据，例如有标签和无标签的数据。

图神经网络（深度学习）的缺点：

1. 计算量大：图神经网络通常需要大量的计算资源，如GPU、TPU等，才能训练大规模的图数据。
2. 可解释性差：由于图神经网络的模型比较复杂，因此可解释性相对较差，即我们很难理解模型为什么做出某个决策。

**6.节点层面可以构造哪些特征？这些特征可以归为哪两类？**

度（节点连接数）、重要度、群集系数

可以归为：节点中心度/重要度、节点的局部邻域拓扑连接结构

**7.简述不同的Node Centrality计算方法？**（‼️）

Degree centrality：计算节点连接数，将邻接矩阵按行或按列求和；

Eigenvector centrality：计算相邻节点的重要度求和；

Betweenness centrality：计算相邻节点之间最短路径必须经过该节点的路径条数与相邻节点之间最短路径条数的比值；（必经之路）

Closeness centrality：计算某个节点到其他节点最短路径之和的倒数；（去哪儿都近）

clusetring coefficient：计算某个节点邻居节点两两相邻路径条数与该节点邻居节点两两组合数的比值，或者计算某个节点邻居节点组成三角形个数与某个节点邻居节点可以组成三角形个数的比值。（邻居节点抱团情况）

**8.只用Node Degree作为节点重要度，会有什么缺点？**

忽略节点重要度

**9.Eigenvector centrality和PageRank有什么异同？**（‼️）

Eigenvector centrality和PageRank都是用于衡量网络中节点的重要性的方法，它们的计算方式都涉及到矩阵运算。

异同点如下：

相同点：

- 两种方法都考虑了节点在网络中的位置，以及节点与其他节点之间的链接关系。
- 两种方法都认为与高度中心性节点相连的节点比与低度中心性节点相连的节点更重要。

不同点：

- 计算方式不同：Eigenvector centrality是基于**节点在网络中的连通性**以及**相邻节点的权重**计算的，而PageRank是基于节点的**入度**和**邻居节点的PageRank值**计算的。
- 算法目标不同：Eigenvector centrality的目标是找到最具影响力的节点，而PageRank的目标是找到最有价值的节点。
- 对节点度数的考虑不同：Eigenvector centrality认为，与高度中心性节点相连的节点比与低度中心性节点相连的节点更重要，而PageRank考虑的是节点的入度，即一个节点的PageRank值取决于指向该节点的其他节点的PageRank值。

总的来说，Eigenvector centrality和PageRank都是网络分析中常用的衡量节点重要性的方法，它们的不同点在于算法的目标和计算方式。在不同的应用场景下，选择合适的算法可以更好地解释和理解节点在网络中的作用。

**10.Betweenness Centrality和Closeness Centrality有什么区别？分别揭示了节点是什么特征？**

Betweenness Centrality是表示某个节点是交通要塞，大部分节点最短路径都会经过该节点；

Closeness Centrality是表示某个节点到其他节点都近；

**11.你认为所有海峡中，哪个海峡的Betweenness Centrality最高？**

马六甲海峡

**12.你认为中国所有城市中，哪个城市的Closeness Centrality最高？**

在中国所有城市中，北京市的Closeness Centrality（紧密中心性）最高的可能性比较大。

Closeness Centrality是网络科学中的一种中心性指标，它用于衡量一个节点到其他所有节点的平均最短路径长度，可以反映该节点在网络中的快速到达能力。在中国的交通网络中，北京市是国家政治、文化、科技和交通中心，具有重要的战略地位。北京市作为中国的首都，拥有全国最为完善的交通网络和较高的城市化水平，其交通运输、信息传递等方面的联系都比较紧密，因此其Closeness Centrality可能比其他城市更高。

当然，这只是一种可能性，实际情况还需要更加具体的分析。此外，Closeness Centrality也可以通过具体的交通网络模型和数据进行计算，以得到更加准确的结果。

**13.湖北到中国任何一个省级行政区，最多跨两个省，说明哪个特征高？**

Betweenness Centrality

这个问题涉及到网络科学中的中心性（Centrality）指标。如果从湖北到任何一个中国的省级行政区最多跨两个省，那么这个省级行政区的“介数中心性”（Betweenness Centrality）指标较高。

介数中心性是网络科学中常用的一种中心性指标，用来衡量网络中的节点在信息传递中的重要性。介数中心性越高的节点，在网络中扮演着更为重要的角色，因为它们在不同节点之间的信息传递中发挥着更为关键的作用。在本问题中，如果一个省份到其他省份的最短路径都需要经过这个省份，那么这个省份的介数中心性就会相对较高。

因此，如果从湖北到任何一个中国的省级行政区最多跨两个省，说明这个省级行政区在中国的交通网络中扮演着重要的角色，其介数中心性相对较高。

**14.你认为你所在城市的地铁站中，哪个地铁站的Closeness Centrality最高？哪个地铁站的Clutering Coefficient最高？**

平安里的Closeness Centrality最高，北新桥的Clutering Coefficient最高

**15.地铁线路连接关系，应该如何表示？（邻接矩阵、连接列表、邻接列表）**

邻接矩阵：任意一个站点与其在同一个地铁线路的其它站点间隔的站点数组成的矩阵；

连接列表：任意一个站点与其存在连接线路的其它站点组成的列表；

邻接列表：任意一个站点及其相邻站点组成的列表；

**16.你认为你的人脉圈中，谁的Clutering Coefficient最高？为什么？**

人脉圈中人际关系最好的人的Clutering Coefficient最高，其关联的人越多

**17.什么是Ego-Network（自我中心网络）？**

图是由唯一的一个中心节点及其邻居节点组成，且仅包含中心节点与邻居节点组成的连接，以及邻居节点间组成的连接。

**18.Graphlet和Wavelet（小波分析）有什么异同？**

小波分析（Wavelet analysis）是一种信号处理技术，用于将信号分解成不同的频率成分，并在不同时间和频率分辨率下进行分析。小波分析是由一个称为小波的基本函数系列进行的，这些函数在时间和频率上都有局部性质。小波函数与傅里叶变换中的正弦和余弦函数不同，小波函数是局部化的，因此可以提供更好的时间和频率分辨率。这使得小波分析在信号处理中更加灵活，可以捕捉到信号的瞬时特性。

小波分析通常分为两个部分：小波分解和重构。在小波分解中，信号被分解成一系列小波函数，每个小波函数代表一个不同的频率范围。在重构中，这些小波函数被组合成原始信号。小波分析可以应用于许多不同的领域，包括信号处理、图像处理、模式识别和数据压缩等。

小波分析的一些优点包括：

1. 时间和频率分辨率更好：小波函数具有局部性，可以提供更好的时间和频率分辨率，因此可以捕捉到信号的瞬时特性。
2. 高效性：小波分析可以在小波域中对信号进行分解和重构，这比频域和时域分析更有效。
3. 适用于非平稳信号：小波分析可以处理非平稳信号，因为它可以适应信号的瞬时频率变化。



Graphlet和Wavelet是两个不同的概念，虽然它们的名字相似，但它们在不同的领域中有着不同的应用。

Graphlet是用于研究图形结构的概念，是一种小的、固定大小的子图，可以通过计算图形中各个Graphlet的数量和分布来描述图形的拓扑结构特征。Graphlet在图形分析、社交网络分析、蛋白质结构研究等领域中被广泛应用。

Wavelet（小波分析）则是一种信号分析方法，用于分析时频域中的信号。它将信号分解为不同的频率分量，通过对每个频率分量进行局部分析，可以获得信号的详细信息。小波分析广泛应用于数字信号处理、图像处理、语音识别等领域。

虽然Graphlet和Wavelet在应用领域上存在差异，但它们在数学理论上有一些相似之处。例如，它们都涉及到计算局部特征，Graphlet在图形中计算局部子图数量，而Wavelet在信号中计算局部频率分量。此外，Graphlet也可以用于图像处理中的特征提取，类似于Wavelet在图像中提取局部频率特征。

**19.由四个节点组成的图，存在多少种Graphlet？**

11种

**20.五个节点构造的所有Graphlet中，存在多少种不同角色的节点？**

21种

**21.节点的哪些特征，可以衡量该节点是否为中心枢纽节点？桥接节点？边缘孤立节点？**(‼️)

在网络分析中，一个节点的中心性（Centrality）通常可以用来衡量该节点在网络中的重要性和影响力。不同类型的中心性指标可以用来衡量节点在网络中的不同方面的重要性，例如连接性、传播能力、控制力等等。下面是一些常用的中心性指标及其对应的特征：

1. 度中心性（Degree Centrality）：该节点与其他节点之间的连接数。节点的度中心性越高，表示该节点在网络中与其他节点有更多的联系，因此更容易成为信息传播和交流的枢纽节点。
2. **介数中心性（Betweenness Centrality）**：该节点在网络中作为信息传递的桥梁的程度。如果该节点在网络中的任意两个节点之间的最短路径必须经过它，那么该节点的介数中心性就越高。这种节点通常被称为**桥接节点**。
3. 接近中心性（Closeness Centrality）：该节点与其他节点之间的平均距离的倒数。节点的接近中心性越高，表示该节点在网络中与其他节点的联系更加紧密，可以更快地传播信息和掌握网络中的动态变化。
4. **特征向量中心性（Eigenvector Centrality）**：该节点与其他重要节点的联系程度。如果一个节点与许多高度中心的节点有联系，则该节点的特征向量中心性会更高。这种节点通常被称为**中心枢纽节点**。
5. **支配中心性（Domination Centrality）**：该节点在网络中对其他节点的控制力。如果该节点可以通过少量的步骤控制网络中的大部分节点，则该节点的支配中心性会更高。这种节点通常被称为**边缘孤立节点**。

**22.除了课程中讲的Centrality之外，还有哪些Centrality指标？（PageRank、Katz Centrality、HITS Hubs and Authorities）**（‼️）

中心性（Centrality）指标是社交网络分析中用来度量节点重要性的一种方法。以下是一些常见的中心性指标：

1. 度中心性（Degree centrality）：节点的度中心性是指它与其他节点之间的**连接数量**。这个指标很容易计算，可以用于度量节点在网络中的重要性。

2. 紧密度中心性（Closeness centrality）：节点的紧密度中心性是指它与其他节点之间的**平均距离**。这个指标可以用来度量节点的影响力、信息传递能力以及网络中心的位置。

3. 介数中心性（Betweenness centrality）：节点的介数中心性是指它在网络中作为**桥梁的程度**。如果一个节点的介数中心性很高，那么它在信息流传播、网络弹性等方面可能有着重要的作用。

4. 特征向量中心性（Eigenvector centrality）：节点的特征向量中心性是指它在**网络中的影响力**。如果一个节点与其他影响力较大的节点相连，则它的特征向量中心性会相应增加。

5. 力中心性（Force-directed centrality）：节点的力中心性是指在一个力导向图（force-directed graph）中，节点的位置对于整个图的稳定性的贡献程度。这个指标可以用来度量节点在整个图的结构和布局中的作用。

6. Katz中心性（Katz Centrality）：是一种网络中**节点重要性**的度量方式，它考虑到节点与其他节点之间的**路径数量以及路径的长度**。Katz中心性原理基于以下假设：与其他节点**相连的节点**比孤立的节点更重要，连接到**重要节点**的节点比连接到不重要节点的节点更重要。

   Katz中心性将节点的中心性定义为所有节点对之间的路径数的加权和。节点对之间的路径数的加权是通过参数alpha和beta来调节的。具体来说，如果两个节点之间的路径较短，则其权重更大；如果两个节点之间的路径较长，则其权重较小。参数**alpha控制路径长度的影响，beta控制每条路径的权重**。

   Katz中心性的计算方式是对节点对之间的路径数进行加权求和。这可以通过矩阵计算来实现，其中路径权重和路径长度的矩阵通过网络邻接矩阵的幂次计算得到。节点的Katz中心性可以通过将每个节点与所有其他节点的路径数相加来计算。

   Katz中心性的优点是可以捕捉节点在网络中的位置，能够反映节点的传播能力和控制力，因此在社交网络和信息传播等领域具有广泛的应用。

7. PageRank中心性：是一个用于衡量网络节点重要性的算法。它是由Google创始人之一拉里·佩奇（Larry Page）和谢尔盖·布林（Sergey Brin）于1998年开发的，用于评估网页的重要性。

   PageRank中心性的原理是基于图论中的**随机游走**模型，即假设一个随机游客在网络中随机跳转，每次跳转都有等概率随机到任何一个节点，然后按照一定概率在当前节点和它的邻居节点中随机选择一个节点进行下一次跳转。这个过程可以看作是一个**马尔可夫过程**，它最终会收敛到一个稳定状态，这个状态就是PageRank中心性。

   在这个状态下，每个节点的PageRank中心性值与该节点的**入度**（指向该节点的链接数量，被引用数量）以及**入度节点的PageRank中心性值**有关。如果一个节点有很多入度，且这些入度节点的PageRank中心性值都很高，那么该节点的PageRank中心性值也会很高。因此，PageRank中心性不仅考虑了节点的连接数量，还考虑了连接到该节点的节点的重要性。

   PageRank中心性在搜索引擎中广泛应用，它不仅可以用于评估网页的重要性，还可以用于评估其他类型的网络节点的重要性，如社交网络中的用户、学术网络中的论文等。

8. HITS Hubs and Authorities：HITS（Hyperlink-Induced Topic Search）是一个用于衡量网络节点重要性的算法，也称为Hubs and Authorities算法。它是由Jon Kleinberg在1998年提出的，用于识别在一个主题或领域中最具有权威性和专业性的网页。

   HITS算法的基本思想是将网页分为两类：**Hubs和Authorities**。Hubs是指**连接到其他有用信息资源的网页**，而Authorities是指**包含有用信息资源的网页**。Hubs和Authorities是相互依存的，一个网页可以是Hubs也可以是Authorities，或者同时是两者。

   HITS算法通过两个矩阵来计算Hubs和Authorities值。第一个矩阵是**连接矩阵**，它记录了网页之间的链接关系。第二个矩阵是**权威矩阵**，它记录了每个网页在特定领域或主题中的权威性。

   HITS算法通过迭代计算来计算每个网页的Hubs和Authorities值。在每次迭代中，先更新Hubs值，然后更新Authorities值。具体来说，Hubs值通过将每个网页的Authorities值相加得到，而Authorities值通过将每个网页的Hubs值相加得到。

   最终，HITS算法将网页按照Hubs值和Authorities值分别排名。排名靠前的网页通常被认为在特定领域或主题中最具有权威性和专业性。

   HITS算法在搜索引擎中广泛应用，它可以帮助搜索引擎确定在特定领域或主题中最具有权威性和专业性的网页，从而提高搜索结果的准确性和质量。同时，HITS算法也可以用于其他类型的网络分析和挖掘任务。

以上这些中心性指标可以用来帮助分析网络中节点的重要性和作用，不同的指标适用于不同的场景和问题。



### 连接层面的特征工程

**目的**：基于已知连接关系，预测未知连接关系

**将连接转化为向量的方法：**

1.直接提取连接的特征，将连接变成D维向量；

2.将连接两端节点的D维向量拼在一起，这样会丢失中间结构信息；

![截屏2023-02-27 下午9.21.43](https://p.ipic.vip/41oydh.png)

**两种预测场景：**

![截屏2023-02-27 下午9.27.21](https://p.ipic.vip/u2gcu1.png)

**基于连接进行预测流程：**

![截屏2023-02-27 下午9.28.05](https://p.ipic.vip/h6fyk8.png)

#### Distance-based feature 基于两节点距离

该方法只看最短路径长度，忽略个数

#### Local neighborhood overlap 基于两节点局部连接信息

##### common_neighbors：两个节点的共同邻居节点个数

##### jaccard_coefficient：两个节点的共同邻居节点数与两个节点所有邻居节点数的比值

##### adamic_adar_index：两个节点的共同邻居节点对应的连接数对数之和的倒数

![截屏2023-02-17 下午1.01.54](https://p.ipic.vip/8s38ux.png)

![截屏2023-02-17 下午1.03.48](https://p.ipic.vip/p23qvq.png)

基于两节点局部邻域信息的缺点：只考虑两节点是否存在相同节点，忽略了两个节点的潜在关系

![截屏2023-02-17 下午1.07.25](https://p.ipic.vip/r047rd.png)

#### Global neighborhood overlap 基于两节点在全图的连接信息

##### 卡兹系数（‼️）

**卡兹系数**：节点u和节点v之间长度为k的路径个数，反映两个节点在全图的连接信息.

![截屏2023-02-17 下午1.13.14](https://p.ipic.vip/kmj63x.png)

![截屏2023-02-17 下午1.13.43](https://p.ipic.vip/lvf726.png)

![截屏2023-02-17 下午1.14.06](https://p.ipic.vip/2uo5fr.png)

![截屏2023-02-17 下午1.14.25](https://p.ipic.vip/bs0lkm.png)

![截屏2023-02-17 下午1.14.54](https://p.ipic.vip/4oqv07.png)

由于邻接矩阵是对称阵，最终得到其实就是矩阵的特征根。

#### 总结

![截屏2023-02-17 下午1.15.33](https://p.ipic.vip/8zra4e.png)

#### 思考题--连接

**1.连接层面，存在哪些数据挖掘任务，有何应用场景？**

可能认识的人（通过现有人之间的关联，预测潜在可能认识的人）

**2.连接层面可以构造哪些特征？这些特征可以归为哪三类？**

Distance-based feature 基于两节点距离特征 

Local neighborhood overlap 基于两节点局部连接信息

Global neighborhood overlap 基于两节点在全图的连接信息

**3.简述Link Prediction的基本流程**

Link Prediction（链路预测）是一种常见的网络分析问题，旨在预测两个节点之间是否存在一条边（链接）。下面是Link Prediction的基本流程：

1. 数据准备：收集网络数据，将其表示为图（节点和边的集合），并将其分为训练集和测试集。
2. 特征提取：从网络中提取有意义的特征来描述节点和它们之间的关系。这些特征可以是基于节点属性（如节点的度数、聚类系数、介数中心性等）或基于图结构（如节点之间的距离、共同邻居等）的。
3. 模型选择：选择一个合适的机器学习模型来预测缺失的边。这可以是基于传统机器学习算法（如逻辑回归、支持向量机等）或深度学习算法（如神经网络、图卷积神经网络等）的。
4. 模型训练：使用训练集对选择的模型进行训练。
5. 模型评估：使用测试集对模型进行评估，计算预测准确率、召回率等指标来评估模型的性能。
6. 预测未来边：使用训练好的模型来预测未来可能存在的边。这可以帮助我们理解网络的结构和功能，探索节点之间的相互作用以及预测网络的演化趋势等。

总之，Link Prediction是一个多方面的问题，需要选择合适的特征、模型和评估指标来解决，以便更好地预测节点之间的连通性。

**4.A和B都知道梅西，C和D都知道同济子豪兄，请问哪对人物更容易产生社交连接。可以用哪个特征解释？**

C和D，因为知道梅西的人太多了，而知道同济子豪兄的人相对少，所以这两个人之间更相似更容易产生社交连接，卡兹系数

**5.两个节点没有共同好友时，可以用什么特征，将连接编码为D维向量？**

当两个节点没有共同好友时，可以使用其他特征来编码链接，以将链接表示为D维向量。以下是几个可能的特征：

1. 路径特征：计算两个节点之间的最短路径，并使用路径中的节点数、路径长度、路径类型等特征来编码链接。例如，可以使用广度优先搜索（BFS）算法计算最短路径，然后将路径长度和路径类型编码为向量的一部分。
2. 相似性特征：计算两个节点之间的相似性度量，例如余弦相似性、Jaccard相似性等，然后使用这些度量来编码链接。例如，可以将余弦相似性和Jaccard相似性作为链接的两个特征。
3. 结构特征：计算两个节点的结构特征，例如节点的度数、聚类系数、介数中心性等，然后使用这些特征来编码链接。例如，可以将两个节点的度数和它们的聚类系数作为链接的两个特征。

以上特征仅是一些示例，实际应用中可能有其他的特征可以使用。无论使用哪种特征，编码的D维向量可以作为机器学习算法的输入，用于预测链接的存在性

**6.简述Katz Index的算法原理**

Katz Index是一种用于节点相似性度量的算法，可以用于社交网络、知识图谱等图结构数据的分析。Katz Index基于矩阵的幂次展开，可以有效地捕捉节点之间的高阶关系。以下是Katz Index的算法原理：

假设我们有一个无向图G=(V,E)，其中V表示节点的集合，E表示边的集合。**Katz Index将节点之间的相似性表示为它们之间的路径的总和**。具体来说，对于节点i和j，Katz Index的计算如下：

$Katz(i,j) = \sum_{l=1}^{\infty} \beta^l \cdot A^l(i,j)$

其中，$A^l(i,j)$表示节点i和j之间长度为l的路径数量，**$\beta$是一个衰减因子（$0<\beta<1$），用于权衡高阶路径和低阶路径的贡献**。通常情况下，**$\beta$的取值越小，考虑的高阶路径越多，但计算量也越大**。

为了避免直接计算无穷级数，我们可以使用矩阵的幂次展开来逼近Katz Index的值。具体来说，我们可以将邻接矩阵A进行幂次展开，得到：

$A^l = \sum_{k=1}^n \sum_{m=1}^n (A^l)_{k,m} e_k e_m^T$

其中，$e_k$和$e_m$是n维向量，表示节点k和节点m在节点集合V中的位置。因此，我们可以将Katz Index表示为：

$Katz(i,j) = \sum_{l=1}^{\infty} \beta^l \cdot \sum_{k=1}^n \sum_{m=1}^n (A^l)_{k,m} e_k(i) e_m(j)$

其中，$e_k(i)$和$e_m(j)$表示向量$e_k$和$e_m$的第i和j个元素。由于幂次展开的系数$(A^l)_{k,m}$随着幂次l的增加而减小，我们可以通过截断无穷级数来近似计算Katz Index的值，即：

$Katz(i,j) \approx \sum_{l=1}^{L} \beta^l \cdot \sum_{k=1}^n \sum_{m=1}^n (A^l)_{k,m} e_k(i) e_m(j)$

其中，L是截断幂次，通常选择一个较小的值。

通过计算Katz Index，我们可以得到每对节点之间的相似性度量，然后可以将其用于节点聚类、链接预测等任务。需要注意的是，由于幂次展开的计算量较大，对于大规模的网络，Katz Index的计算可能会非常耗时。因此，有许多改进的算法被提出，以提高计算效率和精度。其中一种常见的改进方法是使用迭代方法，例如基于幂法或逆迭代法的算法，可以在有限的迭代次数内逼近Katz Index的值。

另外，为了避免邻接矩阵中的噪声和离群点对结果的影响，还可以对邻接矩阵进行正则化处理。一种常见的正则化方法是对邻接矩阵进行归一化，例如将邻接矩阵除以节点的度数，得到一个行归一化的邻接矩阵。

总之，Katz Index是一种基于矩阵幂次展开的节点相似性度量算法，可以捕捉节点之间的高阶关系，并在节点聚类、链接预测等任务中得到广泛应用。虽然Katz Index的计算量较大，但有许多改进的算法可以提高其效率和精度。

**7.如何计算节点U和节点V之间，长度为K的路径个数**

构建邻接矩阵，然后将邻接矩阵进行k次相乘，节点u对应矩阵的行和节点v对应矩阵的列所对应的值

**8.为什么不直接把link两端节点的向量特征concat到一起，作为link的向量特征**

这样会忽略中间结构信息，从而丢失一部分特征



### 全图层面的特征工程

**目标：提取出的特征应反映全图结构特点**

#### 子图匹配-Graphlet Kernel

原理：子图匹配是指在一个大图中找到一个子图，使得这个子图和另一个给定的小图结构相同，即两个图拥有相同的节点和边，并且节点和边之间的关系也相同。

**子图匹配**的原理是在大图中寻找一个与给定小图**同构**的子图。这个过程可以通过**暴力搜索**或者使用**图匹配算法**来实现。

暴力搜索的方法是枚举大图中所有的可能的子图，并且与给定的小图进行比较。这个方法的时间复杂度非常高，因为在大图中可能存在非常多的子图，而且对于每一个子图，需要进行相应的比较操作。因此，这个方法只适用于小规模的图。

**更加高效的方法是使用图匹配算法**，其中最常用的算法是VF2算法。VF2算法是一种基于递归的深度优先搜索算法，它通过递归的方式遍历大图和小图中的节点，并且比较它们之间的关系。VF2算法将图匹配问题分为两个阶段，第一阶段是通过计算节点的相似度来减少搜索空间，第二阶段是使用递归的方式进行子图匹配。

除了VF2算法，还有一些其他的图匹配算法，比如Graph Isomorphism（GI）算法、Subgraph Isomorphism（SI）算法等等，它们都有各自的优缺点，可以根据具体的应用场景选择合适的算法来进行子图匹配。

图中的Bag-of-Words(BoW)方法：将每个词出现的频率作为文档的向量；

![截屏2023-02-16 下午8.32.40](https://p.ipic.vip/6aupsq.png)图中的Bag-of-Node-Degrees方法：用每个节点的度作为向量；

![截屏2023-02-16 下午8.31.55](https://p.ipic.vip/nwf61d.png)

统计全图中子图的个数![截屏2023-02-16 下午8.38.29](https://p.ipic.vip/xkqpxl.png)

11个子图是非同形的

![截屏2023-02-16 下午8.34.54](https://p.ipic.vip/6uepgn.png)

![截屏2023-02-16 下午8.51.32](https://p.ipic.vip/oyf0si.png)

![截屏2023-02-16 下午8.41.21](https://p.ipic.vip/kd43k2.png)

![截屏2023-02-16 下午8.53.27](https://p.ipic.vip/emk3er.png)

当两个图的大小不一致，则需要将图进行归一化，$h_G$表示归一化后的结果。

![截屏2023-03-02 下午7.44.49](https://p.ipic.vip/x7jdfn.png)

由于子图匹配需要消耗大量时间和算力（枚举所有子图），所以很少使用这个算法，时间的复杂度$n^k$，其中n是节点个数，k是子图数量。

#### 颜色微调算法-Weisfeiler-Lehman Kernel

![截屏2023-03-02 下午7.45.58](https://p.ipic.vip/c91npy.png)

![截屏2023-02-16 下午8.58.33](https://p.ipic.vip/jtrmre.png)

![截屏2023-02-16 下午8.58.59](https://p.ipic.vip/h7u46k.png)

![截屏2023-02-16 下午8.59.42](https://p.ipic.vip/x2sfl2.png)

![截屏2023-02-16 下午9.00.15](https://p.ipic.vip/d1v53e.png)

![截屏2023-02-16 下午9.01.06](https://p.ipic.vip/ic3qdc.png)

第一个列表显示错误：应该为[6,2,1,2,1,0,2,1,0,0,2,1,0]

注意：这里的列表是在进行所有的颜色定义的时候，存在对应编码的节点个数，最开始的时候，两张图所有节点都是1，所以两个列表的1对应列表位置的取值为6，依次类推。![截屏2023-02-16 下午9.21.25](https://p.ipic.vip/quhp29.png)

![截屏2023-02-16 下午9.05.24](https://p.ipic.vip/cx181r.png)

方法：将每个节点的颜色hash编码补充到词汇表中，然后统计编码的个数组成向量。

注意：需要统计任意一个图出现的编码才考虑，都没出现不需要考虑。

![截屏2023-02-16 下午9.36.34](https://p.ipic.vip/nhxgma.png)

#### 思考题--子图

 **1.全图层面，存在哪些数据挖掘任务，有何应用场景？**

目标检测，例如：人脸识别、无人驾驶等

**2.全图层面可以构造哪些特征？**

在全图层面，可以构造很多不同类型的特征来表示一个图的属性和结构，以下是一些常用的特征类型：

1. 节点度数特征：节点度数是指与节点相邻的边的数量，度数特征可以表示节点的重要性和中心性，例如，节点的度数越高，说明它在网络中越重要。
2. 子图统计特征：子图统计特征可以描述图中不同的子结构和模式，例如，可以统计图中不同大小的子图的数量、子图的密度等等。
3. 连通性特征：连通性特征可以描述图的连接性质，例如，可以计算图的连通分量的数量、大小、分布等等。
4. 中心性特征：中心性特征可以描述图中节点的中心性质，例如，可以计算节点的介数中心性、紧密中心性、度中心性等等。
5. 图谱特征：图谱特征可以描述图的结构和属性，例如，可以计算图的谱半径、平均路径长度、特征向量中心性等等。
6. 分区特征：分区特征可以描述图的社区结构和聚类性质，例如，可以计算图的模块度、聚类系数等等。

除了上述特征类型之外，还有很多其他类型的特征可以用来表示图的结构和属性，这些特征可以结合具体的应用场景进行选择和设计，以便更好地描述和分析图的属性和结构。同时，特征的选择和设计也需要考虑计算效率和算法可解性等方面的问题。

**3.全图层面的Graphlet，和节点层面的Graphlet，有什么区别？**

全图层面的graphlet和节点层面的graphlet的区别在于，全图层面的graphlet会用到所有节点，而节点层面的graphlet只会使用到部分节点（节点的领域）。

Graphlet是图中的子图，具有一定的拓扑结构特征。在图的分析中，Graphlet是一种常用的局部特征表示方法。

全图层面的Graphlet是指对整个图进行Graphlet分析，即对图中的每个节点对应的子图进行计算。全图层面的Graphlet主要用于对整个图的拓扑结构进行全局特征提取，以及图的分类、聚类、相似性比较等任务。

节点层面的Graphlet是指对每个节点的邻域子图进行Graphlet分析，即对每个节点周围的邻居子图进行计算。节点层面的Graphlet主要用于对节点的拓扑结构进行局部特征提取，以及节点分类、相似性比较、社区检测等任务。

因此，全图层面的Graphlet和节点层面的Graphlet之间主要的区别在于，前者是对整个图进行特征提取，后者则是对每个节点的邻域进行特征提取。两者都可以用于图的分析和挖掘，但应根据具体任务需求选择合适的方法。

**4.子图匹配，算法复杂度如何计算？**

时间的复杂度$n^k$，其中n是节点个数，k是子图数量

**5.简述Weisfeiler-Lehman Kernel的算法原理**

Weisfeiler-Lehman Kernel (WL核) 是一种**基于图同构性质的图形比较方法**，主要用于图形分类、聚类和搜索等问题。它的基本思想是**通过迭代的方式为每个节点构建一个局部的标签，然后利用这些标签来比较图的相似性**。

该算法基于以下两个原则：

1. 相似的节点应该拥有相同的标签。
2. 不同的标签应该对应于不同的节点。

具体来说，算法首先为每个节点赋予一个初始标签，然后对每个节点的邻居节点集合进行哈希处理，将哈希值作为新的标签，更新节点的标签。这个过程可以迭代多次，每次更新后，将新的标签和邻居节点的标签合并并重新哈希，得到新的标签。最终，每个节点的标签将是一个包含多个哈希值的向量。最后，使用这些标签来比较两个图的相似性，可以使用支持向量机（SVM）等方法进行分类或聚类等任务。

总体来说，WL核是一种简单但有效的图形比较方法，可以很好地处理不同类型的图形数据，但在某些情况下可能会出现过拟合等问题。

**6.Weisfeiler-Lehman Kernel的词汇表（颜色表）是如何构建的？**

Weisfeiler-Lehman Kernel（WL Kernel）是一种用于图分类和相似性计算的图形特征提取算法。在这种算法中，词汇表（或者颜色表）是用于迭代过程中标记节点的标签集合。下面是构建词汇表的步骤：

1. 初始化：对于每个节点，将其初始标记设置为节点本身的标签。
2. 迭代：对于每一次迭代，对于每个节点，将其与其所有的邻居节点的标记按字典序排序，并将它们连接成一个字符串。然后，使用哈希函数将该字符串映射到一个新的标签，以更新该节点的标记。对于所有被更新过的节点，将它们的新标记添加到词汇表中。
3. 终止：如果在迭代过程中出现了重复的标签，则停止迭代。此时，词汇表中的标签就是最终的颜色表。

需要注意的是，由于哈希函数的随机性质，词汇表的大小是不确定的。通常情况下，为了保证算法的可重复性，应该使用确定性哈希函数来构建词汇表。

**7.Weisfeiler-Lehman Kernel，算法复杂度是多少？**

Weisfeiler-Lehman Kernel（WL Kernel）是一种用于图形特征提取的算法，用于计算两个图之间的相似性或分类。WL核的时间复杂度取决于两个因素：图的大小和WL迭代的次数。

具体来说，假设我们有两个图G1和G2，它们分别有n1和n2个节点，并且在WL算法中进行了h次迭代。则WL核的时间复杂度为O(h(n1+n2)log(n1+n2))，其中log(n1+n2)是由于在每次迭代中需要将节点的标签排序。

可以看出，WL核的时间复杂度是线性的，即O(n)，而不是图的大小的平方。这使得WL核可以有效地处理大型图，因为它的运行时间不会随着图的大小增加而变得过于昂贵。但是，需要注意的是，在实践中，WL核的常数因子可能比较大，因此需要进行优化以获得更好的性能。

**8.Weisfeiler-Lehman Kernel和图神经网络（GNN）有什么关系？**

Weisfeiler-Lehman Kernel（WL Kernel）和图神经网络（GNN）都是用于图形数据分析和特征提取的方法。它们的主要区别在于，WL核是一种基于**图形同构性的局部算法**，而GNN是一种基于**神经网络的全局算法**。

具体来说，WL核通过迭代计算每个节点的局部特征，然后将这些特征汇总为一个全局特征向量。在每一次迭代中，WL核会比较节点的邻居节点之间的相似性，并将它们的特征进行聚合。这个过程可以看作是一个局部到全局的过程。

相比之下，GNN通过在图上定义一些卷积、池化等神经网络操作，来学习节点的特征表示。与WL核不同的是，GNN使用一些共享的参数来学习全局的特征表示，使得它能够更好地捕捉图形的全局结构和拓扑信息。

尽管WL核和GNN是两种不同的方法，但是它们在某些方面也有一些相似之处。例如，GNN可以通过将图的邻接矩阵作为输入来执行一些基于局部邻域的操作，这与WL核在计算节点特征时所做的操作是类似的。此外，还有一些将GNN和WL核结合起来的方法，例如将GNN的节点特征表示输入到WL核中，或将WL核的颜色表作为GNN的输入特征。这些方法可以提高图形数据分析和特征提取的效果，并为更广泛的应用提供支持。

**9.简述Kernel Methods基本原理**

Kernel方法是一类常用于分类和回归等机器学习问题的算法。其基本原理可以概括为以下几个步骤：

1. 特征映射：将输入的数据点映射到一个高维特征空间中。通常情况下，这个特征空间的维度会比输入数据的维度高得多。
2. 内积计算：在特征空间中，对于任意两个数据点，计算它们的内积。这个内积可以用来表示这两个数据点在特征空间中的相似程度。
3. 核函数：为了避免在高维特征空间中进行复杂的计算，可以使用一个核函数来代替内积计算。核函数是一个对称、正定的函数，它可以计算两个数据点在特征空间中的内积。
4. 分类或回归：根据计算出来的相似程度，对数据点进行分类或回归预测。分类的过程通常使用支持向量机（SVM）算法，而回归的过程则使用核岭回归等算法。

总之，Kernel方法通过特征映射和核函数的使用，可以将低维数据转换到高维空间中进行计算，从而更好地处理非线性问题。它在分类和回归等机器学习问题中具有广泛的应用。

**10.为什么在Graph-level任务中，使用Kernel Methods**

在图级任务中使用核方法的主要原因是因为核方法提供了一种通用的框架，可以在非欧几里德空间中对数据进行处理。对于图形数据，我们可以将图形表示为节点和边的集合，其中节点表示对象，边表示它们之间的关系。

核方法的基本思想是将数据映射到一个高维特征空间中，然后在该空间中计算相似性。这种方法在处理图形数据时非常有用，因为我们可以使用核函数计算节点和边之间的相似性，并将其用作特征向量中的元素。这样，我们可以在高维空间中进行图形分类、聚类、回归等任务。

另外，核方法还具有计算效率高和可扩展性好的优点，这些特点使其在大规模数据集上表现良好。因此，核方法被广泛用于图形分析和机器学习中的其他领域，例如文本分类、图像分类和生物信息学。

**11.除了Graphlet Kernel和Weisfeiler-Lehman Kernel之外，还有哪些Kernel**

在图形领域中，除了Graphlet Kernel和Weisfeiler-Lehman Kernel外，还有许多其他类型的核函数可以用于图形分析和机器学习。以下是其中的一些常见类型：

1. Graph Kernel：基于图形结构的核函数，可以比较两个图形之间的相似性。常用的图核函数包括Subgraph Kernel、Random Walk Kernel、Shortest Path Kernel、Graph Edit Distance等。
2. Node Kernel：基于节点属性的核函数，可以比较两个节点之间的相似性。例如，使用内积或高斯核计算节点特征向量之间的相似性。
3. Edge Kernel：基于边属性的核函数，可以比较两个边之间的相似性。例如，使用内积或高斯核计算边特征向量之间的相似性。
4. Substructure Kernel：基于图形中的子结构的核函数，可以比较两个图形中的子结构之间的相似性。例如，使用路径、子图、环等特定的子结构作为特征，计算它们之间的相似性。
5. Deep Kernel Learning：基于深度学习的核函数，可以学习一个可以比较两个图形之间的相似性的核函数。这种方法通常使用卷积神经网络或图卷积网络来提取图形的特征表示，然后使用内积或高斯核计算它们之间的相似性。

这些核函数各具特点，可以根据不同的应用场景选择合适的核函数来处理图形数据。

**12.传统图机器学习和特征工程中，哪些特征用到了邻接矩阵Adjacency Matrix？** 

Degree centrality

eigenvector centrality

在传统的图机器学习和特征工程中，邻接矩阵（Adjacency Matrix）是一个非常重要的特征，通常用于表示图形的连接关系。邻接矩阵是一个N×N的矩阵，其中N是节点的数量，矩阵中的每个元素A[i][j]表示节点i和节点j之间是否存在边。如果节点i和节点j之间存在边，则A[i][j]=1，否则A[i][j]=0。

邻接矩阵可以用于提取多种图形特征，如下所示：

1. 度数特征：每个节点的度数是指与其相邻的节点数目，可以通过邻接矩阵计算得到。节点的度数可以用于表示节点在图形中的重要性和连通性，例如，高度连通的节点通常具有较高的度数。
2. 子图特征：邻接矩阵可以用于计算图形中的子图个数、大小、密度等特征。例如，使用邻接矩阵计算图形的行列式、特征值和特征向量等特征可以用于描述图形的结构和对称性。
3. 连通性特征：邻接矩阵可以用于计算图形的连通性，例如，使用广度优先搜索或深度优先搜索算法可以遍历图形中的所有节点，从而确定图形的连通分量。
4. 图形匹配特征：邻接矩阵可以用于计算两个图形之间的相似性，例如，使用图形同构算法可以比较两个图形的结构和连接关系。
5. 图形分割特征：邻接矩阵可以用于计算图形的划分，例如，使用谱聚类算法可以将图形分成多个不同的组别，从而提取图形的局部结构和特征。

总之，邻接矩阵是图形分析和机器学习中非常重要的特征，它可以提取图形的多种结构和连接关系特征，并用于许多应用中，如图形分类、聚类、回归、生成等任务。

**13.如何把无向图节点、连接、全图的特征，推广到有向图？**

将无向图扩展为有向图的过程中，最显著的变化是图中的连接变为有向边，也就是说，每条边都有一个指向其起点和终点的方向。因此，我们需要调整我们对节点、连接和全图的特征的定义。

以下是一些有关如何将无向图节点、连接、全图的特征推广到有向图的方法：

1. 节点特征：

在无向图中，节点的度数是指连接该节点的边的数量。在有向图中，节点的出度是指以该节点为起点的有向边的数量，入度是指以该节点为终点的有向边的数量。因此，我们可以将节点的度数扩展为节点的入度和出度。

1. 连接特征：

在无向图中，连接是没有方向的。在有向图中，连接被称为有向边，其具有一个方向，从起点指向终点。因此，我们可以将连接的权重和距离扩展为有向边的权重和距离，并考虑边的方向。

1. 全图特征：

在无向图中，全图的特征通常是由图的密度、连通性和同构性等性质描述的。在有向图中，我们可以扩展这些概念，例如：

- 有向图的密度可以定义为有向边的数量与节点对的数量之比。
- 有向图的强连通性表示对于任意两个节点 u 和 v，存在从 u 到 v 和从 v 到 u 的有向路径。
- 有向图的同构性表示图的结构和边的方向相同，但节点标签可以不同。

总之，在将无向图扩展为有向图时，我们需要重新审视我们对节点、连接和全图的特征的定义，并相应地进行调整。

**14.如何用代码实现Weisfeiler-Lehman Kernel？**

### 拓展阅读

NetworkX-常用图数据挖掘算法：https://networkx.org/documentation/stable/reference/algorithms/index.html

NetworkX-节点重要度算法：https://networkx.org/documentation/stable/reference/algorithms/centrality.html

NetworkX-Clustering算法：https://networkx.org/documentation/stable/reference/algorithms/clustering.html

NetworkX-最短路径算法：https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html























