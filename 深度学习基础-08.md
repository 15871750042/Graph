![截屏2023-03-12 下午2.31.17](https://p.ipic.vip/no0ifv.png)

![截屏2023-03-12 下午2.36.01](https://p.ipic.vip/ah9lv4.png)

![截屏2023-03-12 下午2.32.22](https://p.ipic.vip/nixlt3.png)

### 损失函数

**监督学习**：输入特征x，需要预测结果所属类别y。

**目标**：$min_{\theta}L(y,f(x))$，即通过调整参数$\theta$，使得预测结果和真实结果差异最小（损失函数进行衡量）。

**梳理常见损失函数**：

**L2损失函数**（$L(y,f(x)=(y-f(x))^2)$用于衡量回归问题的损失函数，可导可微分；

**交叉熵损失函数**用于衡量分类问题的损失函数；

![截屏2023-03-12 下午2.53.01](https://p.ipic.vip/jy6iv5.png)

![截屏2023-03-12 下午2.57.43](https://p.ipic.vip/vd0mxh.png)

![截屏2023-03-12 下午2.58.36](https://p.ipic.vip/27txdq.png)

### 梯度下降

![截屏2023-03-12 下午2.58.57](https://p.ipic.vip/4apobk.png)

 由于最终结果可能不是全局最优，而是局部最优的情况，所以需要关注验证集的效果，判断最终是否过拟合。

![截屏2023-03-12 下午3.10.45](https://p.ipic.vip/m7v48n.png)

梯度下降中，如果使用全部数据计算损失函数，然后计算全局的梯度，当数据量较大的时候，会消耗大量资源，因此实际会选取部分样本计算梯度，每次抽取的样本量为batch size。

![截屏2023-03-12 下午3.11.23](https://p.ipic.vip/e86hf6.png) ![截屏2023-03-12 下午3.30.08](https://p.ipic.vip/z8r7jc.png)

![截屏2023-03-12 下午3.32.52](https://p.ipic.vip/8xovkj.png)

### 反向传播

![截屏2023-03-12 下午3.30.47](https://p.ipic.vip/112bky.png)

![截屏2023-03-12 下午3.36.16](https://p.ipic.vip/jzlvp8.png)

![截屏2023-03-12 下午3.39.16](https://p.ipic.vip/0d2ren.png)

![截屏2023-03-12 下午3.39.42](https://p.ipic.vip/2mqdcp.png)

### 非线性

![截屏2023-03-12 下午3.44.31](https://p.ipic.vip/vazlmc.png)

线性变换要求：

- 原点不变；
- 网格线为直线；
- 网格线等间距平行；

矩阵相乘最终还是矩阵，其实还是线性变换，而实际中存在非线性情况，所以需要将线性转化为非线性，因此引入激活函数。

![截屏2023-03-12 下午3.50.52](https://p.ipic.vip/ablggn.png)

![截屏2023-03-12 下午3.51.44](https://p.ipic.vip/8tsjps.png)

![截屏2023-03-12 下午3.52.11](https://p.ipic.vip/tjwltv.png)

### 思考题

什么是机器学习？什么是深度学习？什么是神经网络？它们之间是什么关系？

深度学习在语音、图像、文本领域，取得了哪些进展？

什么是“极大似然估计”？

推导多分类交叉熵损失函数

简述反向传播的基本原理

神经网络中的激活函数有什么用？如果没有，会怎么样？

SGD为什么是全局梯度的“无偏估计”？

Batch size的大小，会如何影响优化路径？

常用的梯度下降优化器都有哪些？各有什么特点？