图机器学习目标：半监督节点分类、直推式学习

图神经网络目标：归纳式学习

直推式学习：用已知类别的节点，预测未知类别的节点

半监督节点分类解决思路：节点特征工程、节点表示学习（图嵌入）、标签传播（消息传播）、图神经网络

![截屏2023-02-26 上午12.38.13](https://p.ipic.vip/fu0568.png)

![截屏2023-02-26 上午12.57.59](https://p.ipic.vip/ro6l4u.png)

![截屏2023-02-26 上午1.00.03](https://p.ipic.vip/coo0va.png)

### 1.标签传播lpa（‼️）

假定：物以类聚，人以群分（具有相似属性特征的节点，更可能相连且具有相同类别）--相似节点间在图中更接近

原理：从已知类别的节点猜未知类别的节点，不断重复这个过程，得到最终不同类别。

流程：

- 初始化（将已知类别节点设定0或1，未知类别节点设定0.5）
- 进行多次迭代（多次进行--根据相连节点的值求平均，确定该节点值）
- 迭代至收敛或迭代一定次数后（有可能不收敛），得到每个节点的类别（设定阈值：节点的值小于0.5为0，大于0.5为1）

![截屏2023-02-26 上午1.13.47](https://p.ipic.vip/h54at2.png)

该方法的缺点：

1）只是用节点的连接信息，未使用节点的属性特征；

2）不保证收敛，当特征值不在-1到1之间，就会发散（类似神经网络中的梯度爆炸）

### 2.iterative classification

![截屏2023-02-26 上午1.18.49](https://p.ipic.vip/7wlzk7.png)

该方法即使用节点连接信息$Z_v$，也使用节点属性信息$f_v$；

$Y_v$是节点的类别向量

![截屏2023-02-26 上午1.23.17](https://p.ipic.vip/jwsgjx.png)

![截屏2023-02-26 下午2.27.21](https://p.ipic.vip/3eu5tz.png)

网页节点实例：

![截屏2023-02-26 下午2.33.10](https://p.ipic.vip/qkfhp1.png)

![截屏2023-02-26 下午2.32.38](https://p.ipic.vip/bqyk8m.png)

上面第一个表示是否英文字母，第二个表示是否电话号码；

上面灰色节点被错误判别为不良网站，需要通过连接信息和节点属性来判断；

![截屏2023-02-26 下午2.37.23](https://p.ipic.vip/zr7dk5.png)

![截屏2023-02-26 下午2.39.57](https://p.ipic.vip/he6uzk.png)

![截屏2023-02-26 下午2.40.23](https://p.ipic.vip/ps9m2q.png)

![截屏2023-02-26 下午2.41.34](https://p.ipic.vip/kuhhyw.png)

![截屏2023-02-26 下午2.42.44](https://p.ipic.vip/nk3lpp.png)

![截屏2023-02-26 下午2.42.09](https://p.ipic.vip/1l08zs.png)

![截屏2023-02-26 下午2.43.18](https://p.ipic.vip/i8udx3.png)

![截屏2023-02-26 下午2.43.50](https://p.ipic.vip/5ljeeg.png)

![截屏2023-02-26 下午2.44.29](https://p.ipic.vip/ccti4x.png)

![截屏2023-02-26 下午2.45.35](https://p.ipic.vip/vun1nz.png)

### 3.correct & smooth（‼️）

![截屏2023-02-26 下午2.47.44](https://p.ipic.vip/i9dd0c.png)

![截屏2023-02-26 下午2.47.32](https://p.ipic.vip/4lq4c8.png)

注意：这里分类器可以任意选择，同时分类器会对所有节点进行分类预测，即便节点的类别已知，会得到每个节点属于每个类别的概率。

![截屏2023-02-26 下午2.49.59](https://p.ipic.vip/ykuuxx.png)

上图中有些节点属于的类别不是特别确切，需要将这些节点进行处理，使得节点归属的类别非常确切。具体处理见下图：

![截屏2023-02-26 下午2.53.08](https://p.ipic.vip/spx7m3.png)

![截屏2023-02-26 下午2.51.17](https://p.ipic.vip/oyy1w8.png)

计算步骤：

1）将不确定程度进行扩散；

2）对最终的预测结果进行平滑；

![截屏2023-02-26 下午2.57.09](https://p.ipic.vip/tfda1s.png)

![截屏2023-02-26 下午2.57.48](https://p.ipic.vip/0gk9xi.png)

![截屏2023-02-26 下午2.58.30](https://p.ipic.vip/266cem.png)

![截屏2023-02-26 下午3.04.57](https://p.ipic.vip/nhjgnx.png)

![截屏2023-02-26 下午3.06.11](https://p.ipic.vip/3l0mb9.png)

![截屏2023-02-26 下午3.06.43](https://p.ipic.vip/6fg4ob.png)

![截屏2023-02-26 下午3.08.03](https://p.ipic.vip/q8n2vb.png)

![截屏2023-02-26 下午3.14.12](https://p.ipic.vip/peybkt.png)

![截屏2023-02-26 下午3.15.33](https://p.ipic.vip/j3h1rh.png)

![截屏2023-02-26 下午3.16.07](https://p.ipic.vip/fxrzzn.png)

![截屏2023-02-26 下午3.16.37](https://p.ipic.vip/xc119b.png)

![截屏2023-02-26 下午3.17.13](https://p.ipic.vip/fbwahv.png)

![截屏2023-02-26 下午3.17.49](https://p.ipic.vip/w5vcuy.png)

![截屏2023-02-26 下午3.18.29](https://p.ipic.vip/6sklio.png)

![截屏2023-02-26 下午3.19.08](https://p.ipic.vip/q95a8p.png)

### 4.masked label prediction

![截屏2023-02-26 下午10.30.49](https://p.ipic.vip/qm897p.png)

自监督学习方法，随机将一些节点的标签去掉为0，然后有已有标签的节点来进行预测，然后不断进行迭代优化，得到自监督学习模型。

### 5.Loopy Belief Propagation

该方法是动态规划算法

![截屏2023-02-26 下午3.25.58](https://p.ipic.vip/a5bf1o.png)

![截屏2023-02-26 下午3.26.22](https://p.ipic.vip/jj8io8.png)

![截屏2023-02-26 下午3.27.33](https://p.ipic.vip/fiucng.png)

![截屏2023-02-26 下午3.29.05](https://p.ipic.vip/kz0t1x.png)

![截屏2023-02-26 下午3.30.32](https://p.ipic.vip/63j3c0.png)

![截屏2023-02-26 下午3.31.37](https://p.ipic.vip/segxbz.png)

![截屏2023-02-26 下午3.32.05](https://p.ipic.vip/2lwv35.png)

![截屏2023-02-26 下午3.32.44](https://p.ipic.vip/hot81t.png)

图中存在环的时候，从不同子图中收到的消息不再独立

![截屏2023-02-26 下午3.34.33](https://p.ipic.vip/s5ezxm.png)

优缺点：

![截屏2023-02-26 下午3.34.48](https://p.ipic.vip/2y7jte.png)

### 思考题

**1.哪些图数据挖掘问题，可以抽象成半监督节点分类问题？**

通过已知节点分类预测未知节点分类

例如：通过现有借贷数据，预测某个人是否会违约

通过现有用户数据，预测某个人是否是黑产

**2.有哪些解决半监督节点分类问题的方法？各有什么特点？（对照表格简述）**

![截屏2023-02-26 上午12.38.13](https://p.ipic.vip/h8njyq.png)

**3.如何理解Transductive Learning（直推式学习）？** 

直推式学习是通过现有数据类别推导未知类别节点的类别

**4.如何理解Inductive Learning（归纳式学习）？**

归纳式学习是直接对未知类别节点进行分类

**5.本讲讲了哪些标签传播和集体分类方法？**

标签传播lpa、interative classification、correct & smooth、loopy belief propagation

**6.如何理解网络中的Homophily？**

具有相似属性特征的节点更可能相连且具有相同类别

**7.简述Label Propagation的算法原理。**

通过节点间连接信息，来预测未知节点类别，该节点类别由周围节点类别决定。

**8.Label Propagation是否用到了节点的属性特征？**

标签传播未使用节点的属性特征

**9.简述Iterative Classification的算法原理。**

Iterative Classification是一种半监督学习方法，旨在利用已经标记好的样本和未标记的样本来训练分类模型。该算法通常应用于文本分类任务。

算法原理如下：

1. 初始化：首先，将所有已标记样本的类别作为初始标签，用这些标签来训练一个分类模型。
2. 扩展：然后，将分类模型应用于所有未标记的样本，以对它们进行分类。对于每个未标记的样本，使用分类器输出的概率分布来指定可能的类别标签，例如，可以选择概率最高的类别作为新的标签。
3. 更新：然后，将新的标签与已有的标签结合，更新所有已标记样本的类别标签，重新训练分类模型。
4. 重复：重复2-3步骤，直到算法收敛或达到预设的迭代次数为止。

通过这种迭代过程，分类模型逐渐适应未标记数据，并逐渐提高分类准确率。Iterative Classification方法可以应用于多分类和二分类任务，并且在许多情况下比其他半监督学习方法表现更好。

**10.Iterative Classification如何使用节点之间的关联信息？**

Iterative Classification（迭代分类）是一种用于图节点分类的算法，它利用节点之间的关联信息来进行分类。

具体来说，算法会从图中随机选择一些节点作为初始标签，并迭代地更新每个节点的标签，直到标签不再改变为止。在每次迭代中，算法会考虑节点的邻居节点的标签信息，并将它们的标签信息加权平均作为当前节点的新标签。这样一来，节点之间的关联信息就被纳入了分类过程中，可以提高分类的准确性。

例如，对于一个社交网络图，每个节点代表一个用户，节点之间的边代表用户之间的关系。我们可以利用这些关系信息来预测每个用户的类别（比如学生、教师、程序员等）。在这种情况下，迭代分类算法会将每个用户的邻居用户的类别信息考虑在内，从而提高分类的准确性。

总之，Iterative Classification算法通过利用节点之间的关联信息，将这些信息纳入分类过程中，从而提高分类的准确性。

**11.为什么Relational Classification和Iterative Classification都不保证收敛？**

Relational Classification和Iterative Classification都是基于图形结构的分类算法。这些算法的基本思想是将节点的标签预测作为一个图形中节点标签的传播过程。具体而言，算法通常开始于对一些节点的标签进行初始化，然后通过一系列迭代将这些标签扩散到其他节点上，直到达到一定的停止条件。

然而，这些算法并不能保证收敛。这是因为在**图形结构中存在复杂的交互作用**，导致算法可能陷入循环，即使经过多次迭代，节点标签仍然在不断地变化，不能达到稳定状态。这种情况可能发生在存在**环形结构的图中**，或者在存在**大量相互依赖的节点**时。

具体而言，Relational Classification和Iterative Classification的收敛性通常依赖于图的特性和算法的参数选择。在实际应用中，需要仔细选择算法参数以确保算法的收敛性，并采取适当的措施来处理可能导致算法不收敛的情况，例如**增加迭代次数、引入惩罚项或更改算法的拓扑结构等**。

**12.简述C & S的基本原理 **

训练一个分类器，分类器对所有节点进行分类预测，然后进行后处理（correct & smooth)，计算各个节点分类概率，将预测的不确定程度（预测误差）进行传播和分散，考虑相邻节点间的差异，对预测结果进行平滑调整，如果相邻节点之间差异较小，则归为一类。

**13. 如何计算Normalized Adjacency Matrix？**

归一化邻接矩阵（Normalized Adjacency Matrix）是一种常用的表示图形结构的矩阵，它可以在图形分析和机器学习任务中使用。

假设我们有一个无向图 $G=(V, E)$，其中 $V$ 是一组节点，$E$ 是一组边，邻接矩阵 $A$ 的大小为 $|V| \times |V|$，其中：<img src="https://p.ipic.vip/367ltv.png" alt="截屏2023-02-26 下午10.55.38" style="zoom: 67%;" />

归一化邻接矩阵是邻接矩阵 $A$ 的一种变换形式，它将矩阵中的每个元素除以节点的度数之和。节点的度数是指节点连接的边的数量。

归一化邻接矩阵的计算公式如下：

![截屏2023-02-26 下午10.56.04](https://p.ipic.vip/hgix08.png)

其中 $D$ 是一个对角矩阵，其对角线上的元素是节点的度数。换句话说，$D_{ii}=\sum_j A_{ij}$。

归一化邻接矩阵的计算过程可以分解为以下几个步骤：

1. 计算度数矩阵 $D$：对角线上的元素是每个节点的度数。
2. 计算度数矩阵的逆矩阵 $D^{-1}$：将度数矩阵 $D$ 中的每个元素取倒数。
3. 计算对称归一化邻接矩阵 $\hat{A}$：使用式子 $\hat{A} = D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ 计算。

下面是一个示例 Python 代码，展示了如何计算归一化邻接矩阵：

```python
import numpy as np

# adjacency matrix
A = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])

# degree matrix
D = np.diag(np.sum(A, axis=1))

# inverse of degree matrix
D_inv = np.linalg.inv(D)

# normalized adjacency matrix
A_hat = D_inv.dot(A).dot(D_inv)

print(A_hat)
```

输出结果为：

```py
[[0.         0.70710678 0.70710678]
 [0.70710678 0.         0.70710678]
 [0.70710678 0.70710678 0.        ]]
```

归一化邻接矩阵 $\hat{A}$ 的值域在 $[0, 1]$ 之间。其中，$\hat{A}*{ij}$ 表示节点 $i$ 和节点 $j$ 之间的归一化权重。如果节点 $i$ 和节点 $j$ 之间没有边相连，则 $\hat{A}*{ij}=0$。如果节点 $i$ 和节点 $j$ 之间有一条边相连，则 $\hat{A}*{ij}$ 的值为归一化的边权，其值等于节点 $i$ 和节点 $j$ 的度数之和的倒数的平方。例如，在上面的示例中，节点 1 和节点 2 之间的边权为 $\hat{A}*{12}=0.70710678$，节点 1 和节点 3 之间的边权为 $\hat{A}_{13}=0.70710678$。节点 2 和节点 3 之间的边权也是 $0.70710678$。

归一化邻接矩阵 $\hat{A}$ 的作用是将不同节点之间的度数差异化处理，并增加不同节点之间的相似度。它常常被用于图形嵌入、聚类、图形分类等任务中。例如，在谱聚类算法中，通常使用归一化邻接矩阵来表示图形的结构，从而得到更好的聚类结果。

**13.如何用图论解释成语“三人成虎”、“众口铄金”？**

当图中有环的时候，来自不同节点的消息彼此不独立，这样就会是某个消息进行加强，进而达到“三人成虎”、“众口铄金”

**14.如何用图论解释《三体》中的“猜疑链”？**

当节点消息传递存在多阶过程，我认为你觉得他是什么样的？这就会形成猜疑链

**15.简述Loopy Belief Propagation的基本原理。**

Loopy Belief Propagation是一种用于概率图模型推断的近似算法，适用于具有环路（loopy）的图。它基于信念传播（belief propagation）算法，但允许在图上迭代多次以获取更准确的近似解。

Loopy Belief Propagation算法的基本原理如下：

1. 初始化：为每个节点和边缘变量（即与节点相邻的边的概率变量）设置一个初始值。
2. 传递消息：从每个节点开始，沿着它连接的每个边缘变量向下一个节点传递消息。消息的值是当前节点到边缘变量的条件概率分布，并且是由当前节点收到的来自其他边缘变量的消息的乘积。
3. 更新边缘变量：当每个节点收到所有相邻节点的消息后，它可以使用这些消息来计算与其相邻的边缘变量的新值。
4. 迭代传递消息和更新边缘变量：重复步骤2和3，直到收敛为止。收敛的判断条件通常是当新的消息和旧的消息之间的差异小于某个阈值时。
5. 计算后验概率：根据更新的边缘变量计算目标变量的后验概率分布。在计算后验概率时，可以使用边缘变量的乘积，对所有变量进行积分或求和。

总的来说，Loopy Belief Propagation算法的核心是在图上迭代传递消息和更新边缘变量。它是一种近似算法，可以有效地处理大规模的概率图模型。

**16.简述Masked Label Prediction的基本原理。**

自监督学习方法，随机将一些节点的标签去掉，然后有已有信息来进行预测这些节点，然后在自监督场景下不断进行迭代优化，得到自监督学习模型。

Masked Label Prediction是一种用于自然语言处理的任务，其基本原理是在一个句子中将一些单词替换为特殊的标记（如[MASK]），然后利用模型预测这些被替换的单词。这种方法可以被用于训练语言模型，因为模型需要理解上下文来正确地预测被替换的单词。

具体来说，Masked Label Prediction包括以下步骤：

1. 将句子中的一些单词随机选择并用[MASK]替换。
2. 将句子输入模型中进行处理，模型会对被替换的单词进行预测。
3. 模型将预测的结果与真实的单词进行比较，通过损失函数计算误差并进行反向传播，以更新模型参数。
4. 重复执行以上步骤，直到模型收敛。

通过Masked Label Prediction训练出的语言模型可以被用于各种自然语言处理任务，如文本分类、命名实体识别、机器翻译等。

**17.Masked Label Prediction可以是Inductive（归纳式）的吗？可以泛化到新来的节点吗？**

Masked Label Prediction（MLP）是一种自监督学习方法，通常用于训练自然语言处理模型。在这种方法中，模型通过预测一个文本序列中的一些词语（被掩盖的）来学习语言模型。

从理论上讲，MLP可以是归纳式的，因为它可以使用先前学到的知识来泛化到新的示例。具体来说，MLP可以通过学习一组通用的语言规则和特征来泛化到新的文本示例。这些规则和特征可以是基于语法、语义、词汇等方面的。

然而，要实现归纳式的泛化，MLP需要在训练时接触到足够多的不同类型的文本示例。这样，模型才能从这些示例中学到通用的规则和特征，从而可以泛化到新的文本示例。如果训练集过于局限，模型可能无法学习到足够通用的规则和特征，从而无法实现归纳式的泛化。

总之，MLP理论上可以是归纳式的，但是否可以泛化到新的节点取决于训练数据的多样性和质量。

**18.本讲的方法，与图神经网络相比，有何异同和优劣？**

图机器学习和图神经网络都是针对图数据的机器学习方法，但是它们有一些不同点和优缺点。

异同点：

1. 表示方法不同：图机器学习通常使用基于特征工程的表示方法，例如度中心性、聚集系数等，而图神经网络则使用基于图卷积神经网络（GCN）、图注意力网络（GAT）等方法进行表示学习。
2. 采样方法不同：在处理大型图时，两种方法采用的采样方法也不同。图机器学习通常采用随机游走、节点采样等方法，而图神经网络采用的方法则包括全局池化、局部采样等。
3. 计算复杂度不同：由于使用不同的表示方法和采样方法，图机器学习和图神经网络的计算复杂度也有所不同。

优缺点： 图机器学习的优点包括：

1. 可解释性强：基于特征工程的方法可以提供可解释的特征表示，使得模型的结果更容易理解和解释。
2. 稳定性好：基于特征工程的方法不容易受到数据的扰动影响，对噪声数据的容错能力较强。
3. 计算复杂度相对较低：采用基于特征工程的方法，不需要进行大量的计算，因此计算复杂度相对较低。

缺点包括：

1. 特征提取困难：由于图结构通常比较复杂，因此特征提取过程比较困难。
2. 可扩展性差：基于特征工程的方法不太适用于大规模图结构，对于包含大量节点和边的图结构，特征提取和处理会变得非常困难。

图神经网络的优点包括：

1. 可以自动学习特征表示：图神经网络可以自动学习节点和边的特征表示，无需手动进行特征工程。
2. 精度高：图神经网络能够处理复杂的非线性图形结构，因此在许多任务中能够获得比传统方法更高的精度。
3. 可扩展性好：图神经网络可以处理大规模图结构，并且具有较好的可扩展性。

缺点包括：

1. 计算复杂度高：由于需要进行大量的计算，因此图神经网络的计算复杂度较高，对计算资源的要求比较高。
2. 可解释性差：

**19.如何重新理解“出淤泥而不染”？**

**20.如何重新理解“传销”、“病毒式裂变”？**